---
output:
  md_document:
    variant: markdown_github
  pdf_document: default
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

This document is a first pass at describing a grammar of modelling for supervised learning with a focus on prediction.

```{r, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
library(tidyverse)

data(Boston, package = "MASS")
boston <- as_tibble(Boston)
boston
```

# Motivation

Each model in R essentially lives in its own package and has a unique interface. This introduces a large amount of cognitive load on data analysts. For example, suppose we want to use KNN. We might do something like this:

```{r}
library(class)
library(e1071)

train <- boston %>% 
  slice(1:400)

test <- boston %>% 
  slice(401:506) %>% 
  select(-medv)

knn_preds <- knn(test = test,
                 train = select(train, -medv),
                 cl = train$medv,
                 k = 5)
```

But if we want to use naive Bayes, we might end up writing code that looks like this:

```{r}
nb_model <- naiveBayes(medv ~ ., data = train)
nb_preds <- predict(nb_model, newdata = test, type = "class")
```

This has some problems:

  - `knn` generates predictions immediately on a test set, while `naiveBayes` creates a model object
  - `knn` and `naiveBayes` have different interfaces for specifying design matrices and outcomes
  - for `knn` we have to pass arguments `cl` and `k` even though it would be reasonable to select `k` by cross-validation and `cl` could be more succinctly expressed as an outcome in a formula
  - the predictions from `naiveBayes` object are not by default the same type as the outcome the user inputs
  
That is, there isn't a consistent interface to the packages themselves, and the packages don't provide a conceptual framework that makes it easy to think about modelling.

Before proposing an interface for modelling, I think it's worthwhile to step back and define the objects that we'd like to work with and some reasonable actions we should be able to perform with those objects.

# A grammar of supervised learning

When someone talks about supervised learning, they might say something like: "I fit K Nearest Neighbors on the dataset and got 57 percent accuracy." This communicates the gist of what they did, but this isn't enough information to reproduce what they actually did: we don't know what value of $k$ or the distance metric they used.

When someone says that they used K Nearest Neighbors, they are referring to a **family of supervised learning models**, that is, KNN with $k \in \{1, 2, ...\}$ and some distance metric from a list of metrics. Together all the possible values of $k$ combined with all the possible metrics form a **hyperparameter space**, and we hope that our friend has selected hyperparameters in some reasonable way.

Once our friend has selected hyperparameters, say $k=13$ and Euclidean distance as a metric, they are now speaking about a specific, unambiguous **model**. Once someone gives us these details, we have enough information to reproduce the training process ourselves.

These objects form the basis for our grammar:

- `model family` = `modelling technique` + `hyperparameter space`
- `model` = `modelling technique` + `specific values of hyperparameters` 

As a concrete example, consider the `glmnet` package.

- `glmnet` objects fit with a specific value of $\lambda$ correspond to `model` objects
- `cv.glmnet` objects correspond to `model family`s containing performance information for various hyperparameter values

There are some important differences between `model family` objects and `model` objects. If we have some data, we know how to fit a `model`. With a `model family` that's less obvious - we have to specify some hyperparameter selection method because it isn't feasible to the train `model`s for all possible valuables of the hyperparameters. Additionally, we typically perform inference on `model`s rather than `model family`s.

In practice what we'd like to do is specify a set of hyperparams and train `model` objects for sets of hyperparameter combinations. Together this set of models will form the `model family`. Each of the `models` contained in the `model family` will have difference predictive performance, so an important part of the model family is a specification of a performance metric (i.e. RMSE) and a performance assess strategy (i.e. cross-validation).

Keeping `model`s and `model family`s in mind, let's think about how existing machine learning libraries work.

## `Scikit-Learn`

In Scikit-Learn we can fit and predict with `model` objects quite intuitively:

```{r}
from sklearn import neighbors
knn = neighbors.KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)
predictions = knn.predict(X_test)
```

However, I find the abstractions to work with `model family`s less satisfying. Considering the KNN model family where we use random search to select a value of $k$, which looks something like:

```{r}
hyperparameter_space = {
    'n_neighbors': sp_randint(1, 31)  # pick k in [1, 2, ..., 30]
}

knn = RandomizedSearchCV(KNeighborsClassifier(),
                         param_distributions=hyperparameter_space)

knn.fit(X, y)
```

I find this suboptimal for a couple reasons:

- We have to manually specify the hyperparameter space even though there are sane defaults
- If we wanted to use grid search or a different hyperparameter selection method, we'd have to change the specification of the hyperparameter space (i.e. we'd have to pass a list of values for `n_neighbors` rather than a distribution function for a grid search, etc, etc)
- This doesn't allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren't doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select $k$. Instead we recalculate pairwise distances for each $k$, which is inefficient. Similarly, for penalized regression, we have to refit the model for each $\lambda$ rather than efficiently fitting an entire solution path all at once.

That is, the KNN model family abstraction is closely tied to the hyperparameter selection technique, when it would be cleaner to specify that we want to work with the KNN model family and then plug in whichever hyperparameter selection technique we choose.

There are some things that `Scikit-Learn` does really well though:

- It has an extremely consistent interface that has been established as a communal standard within the Python community
- There are extension libraries that make automated machine learning and ensemble creation easy and pleasant

Lastly, it's worth noting that in `Scikit-Learn` you have instantiate a `KNeighborsClassifier` object and afterward call `fit` on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single class along the lines of 

```{r}
knn_model <- knn_classifer(y ~ ., data, k = 5)
```


## `caret`

In my mind, the `caret` library most closely matches my intuition about working with `model family`s rather than `model`s.

```{r}
library(caret)

# specify 10-fold CV repeated 10 times are hyperparameter selection strategy
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

knn_model <- train(y ~ .,
                   data = train_df, 
                   method = "knn", 
                   trControl = fitControl)
```

Some things I like about the `caret` interface:

- The hyperparameter selection strategy is an argument to the fit method
- Reasonable defaults are provided for some hyperparameters, so it occasionally does feel like you're working with a `model family` that has abstracted away hyperparameter selection
- `caret` takes advantage of built in, smart hyperparameter selection like `cv.glmnet` instead of manually checking values of $\lambda$

Some things I don't like about the `caret` interface:

- The default hyperparameter search is not an extensive enough to ignore hyperparameter selection in the majority of cases and so you end up specifying a tuning grid most of the time. Ideally I think all models should function with default hyperparameter selection as in `h2o.automl()`
- The result of `train` is an object of class `train`. That is, you know it's a model family object, but the precise modelling technique is stored as a string. This means you have to write wrapper code if you want to add a model to the `caret` interface (i.e. it's a nice package to work with, but it probably isn't a great API to set as a communal standard for scientist producing packages)
- Ensembling is hard. There's the `caretEnsemble` extension but I find the interface a messy.

It's also worth noting that `caret` doesn't require object instantiation like `Scikit-Learn` does.

## `mlr`

Briefly, the `mlr` library is similar to `caret`, but with the following interface:

```{r}
task = makeClassifTask(data = iris, target = "Species")
lrn = makeLearner("classif.lda")

n = nrow(iris)
train.set = sample(n, size = 2/3*n)
test.set = setdiff(1:n, train.set)

model = train(lrn, task, subset = train.set)

pred = predict(model, task = task, subset = test.set)
performance(pred, measures = list(mmce, acc))
```

I think this interface has issues similar to `caret`s interface. In particular, I am strongly against manually specifying the learning task and think that the task at hand should be inferred from the class of the data object. I think prediction should be model-centric rather than outcome centric.

## Idiomatic R code for single model

Let's consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so

```{r}
library(lasso)

lasso_object <- lasso(X, y, lambda = 0.01)
predict(lasso_object, X_new)
```

Since there are efficient ways to cross-validate $\lambda$ for lasso regression, the package would likely also implement an interface like

```{r}
lasso_cv_object <- lasso_cv(X, y)
predict(lasso_cv_object)
```

that would automatically select an optimal value of $\lambda$. A nice package author would make `lasso` and `lasso_cv` generics and would also implement formula or even [recipe](https://github.com/topepo/recipes) interfaces to the model, like so

```{r}
lasso_object <- lasso(y ~ ., data = df, lambda = 0.01)
lasso_cv_object <- lasso_cv(y ~ ., data = df)
```

I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because:

- When there isn't a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with

- Unless there's a `recipe` interface to the `lasso_cv` function there isn't a way to do principled preprocessing when resampling to estimate prediction error

- Most of the time you work with multiple models, so it is incredible convenient to be able to do something like:

```{r}
model_familys <- list(lasso(), ridge(), OLS())
train_models <- map(models, ~fit(model_familys, y ~ ., data))
```

but this isn't possible because each function has it's own version of `fit`.

# Initial pass at an interface for modelling in R

Okay, now that we've taken a look at some interfaces and how they well they match up with the `model`/`model family` conception of modelling, let's imagine an interface that makes operations on `model` and `model family`s feel natural in R.

Let's recall what we want out of a modelling interface:

- Work primarily with `model family` objects, where hyperparameter selection is abstracted as far away from the user as possible
- Scientists can provide smart cross-validation schemes when appropriate
- Users can easily select and interchange hyperparameter selection methods or specify their own
- Because hyperparameter / modelling technique specific settings are handled with reasonable defaults, users can easily work with large numbers of models are the same time via a consistent and unified interface
- Ensembling is easy
- Tidy and pipeable data structures

## Proposal (tentative start)

Since the `Scikit-Learn` interface is likely the most uniform and wide known interface, I think it's a good idea to use language from `Scikit-Learn` as much as possible.

```{r}
# object of class "knn". would like this to be cleaner
knn <- new_knn()

# fit model with specific values of hyperparameters. class c("knn", "model")
knn_model <- fit(knn, design, data, k = 13, metric = "euclidean")

# get hyperparameters via reasonable default. class c("knn", "model_family")
# design/data combo should work for:
#   - X, y (matrix, vector)
#   - formula, data frame
#   - recipe, data frame
#
# multiple dispatch would be nice here, curious how to implement
knn_family <- fit(knn, design, data)

# get the best knn model. class c("knn", "model")
best_knn_model <- extract_model(knn_family)

# predictions are of same type as outcome. i.e. numeric outcome 
# gives numeric predictions, factor outcome gives factor predictions
# the following would behavior equivalently
predictions <- predict(knn_family, newdata)
predictions <- predict(best_knn_model, newdata)

# for consistency with scikit-learn and overall sanity, for classification
class_probs <- predict_proba(knn_family, newdata)
```

`model_family` fields:

- `hp_results`: tibble of hyperparameters and resulting performance
- `hp_space`: specification of hyperparameter space
- `hp_strategy`: object like `trainControl` to specify hyperparameter search
- `best_model`: best model from hyperparameter search once trained

I don't have good ideas on how to specify `hp_space` at the moment.

I also think the following would highly increase usability

```{r}
bagged_model  <- bag(lasso(), ridge(), OLS(), n = 50)
stacked_model <- stack(lasso(), ridge(), OLS(), metalearner = GLM())
boosted_model <- boost(lasso(), ridge(), OLS(), loss = "some_loss")
```

There are other important usability components that I think are being developed elsewhere:

- `recipes` package for specifying preprocessing pipeline
- `yardstick` package for tidy metrics
- `tidyposter` package for comparing trained models

## Extensions to unsupervised learning

This interface could also be extended to unsupervised learning following the spirit of `Scikit-Learn` by replacing the `predict` method with `transform` in the unsupervised case.

`cv_model` metalearner default grids

## Pipelines?

Could just create supervised learning steps

### Special cases

#### Non-repeatable mappings

#### Invertible Mappings

related to `recipes` but different

it would nice to have a standard (S3 method?) for invertible mappings in the spirit of scale/unscale
n some sense recipes does this, but only for the forward direction
1cale/unscale store information about the transform in the returned object, which i'm not sold on
1ut what about something like

p# TODO: implement PCA example transformation, smart IRLBA/non-IRLBA selection

supervised learning: models
unsupervised learning: ??? transformation?

clustering: cv equivalent to select number of clusters and average k-means assignments or penalty parameter for convex clustering

```{r}
pca_trans <- pca(X)
Z <- forward(pca_trans, X)
X_recovered <- backward(pca_trans, Z)
```

klearn does this with the fit/fit_transform methods but i don't think provides a standard for the inverse mapping

