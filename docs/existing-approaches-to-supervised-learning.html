<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Some thoughts on modelling in R</title>
  <meta name="description" content="Some thoughts on modelling in R">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Some thoughts on modelling in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Some thoughts on modelling in R" />
  
  
  

<meta name="author" content="Alex Hayes">


<meta name="date" content="2017-12-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="objects-in-a-grammar-of-supervised-learning.html">
<link rel="next" href="proposed-interface-for-supervised-learning.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Motivation</a></li>
<li class="chapter" data-level="2" data-path="objects-in-a-grammar-of-supervised-learning.html"><a href="objects-in-a-grammar-of-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Objects in a grammar of supervised learning</a></li>
<li class="chapter" data-level="3" data-path="existing-approaches-to-supervised-learning.html"><a href="existing-approaches-to-supervised-learning.html"><i class="fa fa-check"></i><b>3</b> Existing approaches to supervised learning</a><ul>
<li class="chapter" data-level="3.1" data-path="existing-approaches-to-supervised-learning.html"><a href="existing-approaches-to-supervised-learning.html#scikit-learn"><i class="fa fa-check"></i><b>3.1</b> <code>Scikit-Learn</code></a></li>
<li class="chapter" data-level="3.2" data-path="existing-approaches-to-supervised-learning.html"><a href="existing-approaches-to-supervised-learning.html#caret"><i class="fa fa-check"></i><b>3.2</b> <code>caret</code></a></li>
<li class="chapter" data-level="3.3" data-path="existing-approaches-to-supervised-learning.html"><a href="existing-approaches-to-supervised-learning.html#mlr"><i class="fa fa-check"></i><b>3.3</b> <code>mlr</code></a></li>
<li class="chapter" data-level="3.4" data-path="existing-approaches-to-supervised-learning.html"><a href="existing-approaches-to-supervised-learning.html#idiomatic-modelling-in-r"><i class="fa fa-check"></i><b>3.4</b> Idiomatic modelling in R</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="proposed-interface-for-supervised-learning.html"><a href="proposed-interface-for-supervised-learning.html"><i class="fa fa-check"></i><b>4</b> Proposed interface for supervised learning</a><ul>
<li class="chapter" data-level="4.1" data-path="proposed-interface-for-supervised-learning.html"><a href="proposed-interface-for-supervised-learning.html#what-we-want-out-of-a-modelling-interface"><i class="fa fa-check"></i><b>4.1</b> What we want out of a modelling interface</a></li>
<li class="chapter" data-level="4.2" data-path="proposed-interface-for-supervised-learning.html"><a href="proposed-interface-for-supervised-learning.html#tentative-proposal"><i class="fa fa-check"></i><b>4.2</b> (Tentative) Proposal</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="extension-to-unsupervised-learning.html"><a href="extension-to-unsupervised-learning.html"><i class="fa fa-check"></i><b>5</b> Extension to unsupervised learning</a><ul>
<li class="chapter" data-level="5.1" data-path="extension-to-unsupervised-learning.html"><a href="extension-to-unsupervised-learning.html#pipelines"><i class="fa fa-check"></i><b>5.1</b> Pipelines</a></li>
<li class="chapter" data-level="5.2" data-path="extension-to-unsupervised-learning.html"><a href="extension-to-unsupervised-learning.html#non-repeatable-mappings"><i class="fa fa-check"></i><b>5.2</b> Non-repeatable mappings</a></li>
<li class="chapter" data-level="5.3" data-path="extension-to-unsupervised-learning.html"><a href="extension-to-unsupervised-learning.html#invertible-mappings"><i class="fa fa-check"></i><b>5.3</b> Invertible Mappings</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="utilities-and-other-important-functionality.html"><a href="utilities-and-other-important-functionality.html"><i class="fa fa-check"></i><b>6</b> Utilities and other important functionality</a></li>
<li class="chapter" data-level="7" data-path="how-to-make-this-happen.html"><a href="how-to-make-this-happen.html"><i class="fa fa-check"></i><b>7</b> How to make this happen</a><ul>
<li class="chapter" data-level="7.1" data-path="how-to-make-this-happen.html"><a href="how-to-make-this-happen.html#roadblocksneeds-before-standardizing-a-modelling-interface-to-community-standardhow-to-implement-new-models"><i class="fa fa-check"></i><b>7.1</b> Roadblocks/needs before standardizing a modelling interface to community standardHow to implement new models</a></li>
<li class="chapter" data-level="7.2" data-path="how-to-make-this-happen.html"><a href="how-to-make-this-happen.html#infrastructure-to-provide"><i class="fa fa-check"></i><b>7.2</b> Infrastructure to provide</a></li>
<li class="chapter" data-level="7.3" data-path="how-to-make-this-happen.html"><a href="how-to-make-this-happen.html#misc"><i class="fa fa-check"></i><b>7.3</b> Misc</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Some thoughts on modelling in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="existing-approaches-to-supervised-learning" class="section level1">
<h1><span class="header-section-number">Part 3</span> Existing approaches to supervised learning</h1>
<div id="scikit-learn" class="section level2">
<h2><span class="header-section-number">3.1</span> <code>Scikit-Learn</code></h2>
<p>In Scikit-Learn we can fit and predict with <code>model</code> objects quite intuitively:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn <span class="im">import</span> neighbors
knn <span class="op">=</span> neighbors.KNeighborsClassifier(n_neighbors<span class="op">=</span>k)
knn.fit(X_train, y_train)
predictions <span class="op">=</span> knn.predict(X_test)</code></pre></div>
<p>However, I find the abstractions to work with <code>model family</code>s less satisfying. Considering the KNN model family where we use random search to select a value of <span class="math inline">\(k\)</span>, which looks something like:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">hyperparameter_space <span class="op">=</span> {
    <span class="st">&#39;n_neighbors&#39;</span>: sp_randint(<span class="dv">1</span>, <span class="dv">31</span>)  <span class="co"># pick k in [1, 2, ..., 30]</span>
}

knn <span class="op">=</span> RandomizedSearchCV(KNeighborsClassifier(),
                         param_distributions<span class="op">=</span>hyperparameter_space)

knn.fit(X, y)</code></pre></div>
<p>I find this suboptimal for a couple reasons:</p>
<ul>
<li>We have to manually specify the hyperparameter space even though there are sane defaults</li>
<li>If we wanted to use grid search or a different hyperparameter selection method, we’d have to change the specification of the hyperparameter space. That is, We’d have to pass a list of values for <code>n_neighbors</code> rather than a distribution function for a grid search.</li>
<li>This doesn’t allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren’t doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select <span class="math inline">\(k\)</span>. Instead we recalculate pairwise distances for each <span class="math inline">\(k\)</span>, which is inefficient.</li>
</ul>
<p>In some cases <code>sklearn</code> provides work arounds to this, for example, with RIDGE regression:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn <span class="im">import</span> linear_model
reg <span class="op">=</span> linear_model.RidgeCV(alphas<span class="op">=</span>[<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">10.0</span>])
reg.fit([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]], [<span class="dv">0</span>, .<span class="dv">1</span>, <span class="dv">1</span>])       
RidgeCV(alphas<span class="op">=</span>[<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">10.0</span>], cv<span class="op">=</span><span class="va">None</span>, fit_intercept<span class="op">=</span><span class="va">True</span>, scoring<span class="op">=</span><span class="va">None</span>,
    normalize<span class="op">=</span><span class="va">False</span>)</code></pre></div>
<p>Here we fit a <code>RidgeCV</code> object which efficiently performs cross-validation on the regularization parameter. However, now we have to remember to call <code>RidgeCV</code> rather than the standard grid search wrapper.</p>
<p>It would be cleaner to specify that we want to work with the KNN/RIDGE model family and then plug in whichever hyperparameter selection technique we choose via the same interface.</p>
<p>There are some things that <code>Scikit-Learn</code> does really well though:</p>
<ul>
<li>It has an extremely consistent interface that has been established as a communal standard within the Python community</li>
<li>There are extension libraries that make automated machine learning and ensemble creation easy and pleasant</li>
</ul>
<p>Lastly, it’s worth noting that in <code>Scikit-Learn</code> you have to instantiate a <code>KNeighborsClassifier</code> object and afterward call <code>fit</code> on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single call, like so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn_model &lt;-<span class="st"> </span><span class="kw">knn_classifer</span>(y <span class="op">~</span><span class="st"> </span>., data, <span class="dt">k =</span> <span class="dv">5</span>)</code></pre></div>
</div>
<div id="caret" class="section level2">
<h2><span class="header-section-number">3.2</span> <code>caret</code></h2>
<p>In my mind, the <code>caret</code> library most closely matches my intuition about working with <code>model family</code>s rather than <code>model</code>s.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)

<span class="co"># specify 10-fold CV repeated 10 times are hyperparameter selection strategy</span>
fitControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,
                           <span class="dt">number =</span> <span class="dv">10</span>,
                           <span class="dt">repeats =</span> <span class="dv">10</span>)

knn_model &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,
                   <span class="dt">data =</span> train_df, 
                   <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, 
                   <span class="dt">trControl =</span> fitControl)</code></pre></div>
<p>Some things I like about the <code>caret</code> interface:</p>
<ul>
<li>The hyperparameter selection strategy is an argument to the fit method</li>
<li>Reasonable defaults are provided for some hyperparameters, so it occasionally does feel like you’re working with a <code>model family</code> that has abstracted away hyperparameter selection</li>
<li><code>caret</code> takes advantage of built in, smart hyperparameter selection like <code>cv.glmnet</code> instead of manually checking values of <span class="math inline">\(\lambda\)</span></li>
</ul>
<p>Some things I don’t like about the <code>caret</code> interface:</p>
<ul>
<li>The default hyperparameter search is not an extensive enough to ignore hyperparameter selection in the majority of cases and so you end up specifying a tuning grid most of the time. Ideally I think all models should function with default hyperparameter selection as in <code>h2o.automl()</code></li>
<li>The result of <code>train</code> is an object of class <code>train</code>. That is, you know it’s a model family object, but the precise modelling technique is stored as a string. This means you have to write wrapper code if you want to add a model to the <code>caret</code> interface (i.e. it’s a nice package to work with, but it probably isn’t a great API to set as a communal standard for scientist producing packages)</li>
<li>Ensembling is hard. There’s the <code>caretEnsemble</code> extension but I find the interface a messy.</li>
<li>I find the names of several functions and function arguments unintuitive.</li>
</ul>
<p>It’s also worth noting that <code>caret</code> doesn’t require object instantiation like <code>Scikit-Learn</code> does.</p>
</div>
<div id="mlr" class="section level2">
<h2><span class="header-section-number">3.3</span> <code>mlr</code></h2>
<p>Briefly, the <code>mlr</code> library is similar to <code>caret</code>, but with the following interface:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">task =<span class="st"> </span><span class="kw">makeClassifTask</span>(<span class="dt">data =</span> iris, <span class="dt">target =</span> <span class="st">&quot;Species&quot;</span>)
lrn =<span class="st"> </span><span class="kw">makeLearner</span>(<span class="st">&quot;classif.lda&quot;</span>)

n =<span class="st"> </span><span class="kw">nrow</span>(iris)
train.set =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dt">size =</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span><span class="op">*</span>n)
test.set =<span class="st"> </span><span class="kw">setdiff</span>(<span class="dv">1</span><span class="op">:</span>n, train.set)

model =<span class="st"> </span><span class="kw">train</span>(lrn, task, <span class="dt">subset =</span> train.set)

pred =<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">task =</span> task, <span class="dt">subset =</span> test.set)
<span class="kw">performance</span>(pred, <span class="dt">measures =</span> <span class="kw">list</span>(mmce, acc))</code></pre></div>
<p>I think this interface has issues similar to <code>caret</code>s interface.</p>
<p>Most importantly, however, I believe that the primary focus of supervised learning should be <code>model family</code>s, as opposed to <code>task</code>s. I strongly believe the task at hand should be inferred from the class of the data object.</p>
<p>I think this is important because we often think about inference in terms of <code>model</code>s but almost never in terms of <code>task</code>s.</p>
</div>
<div id="idiomatic-modelling-in-r" class="section level2">
<h2><span class="header-section-number">3.4</span> Idiomatic modelling in R</h2>
<p>Let’s consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lasso)

lasso_object &lt;-<span class="st"> </span><span class="kw">lasso</span>(X, y, <span class="dt">lambda =</span> <span class="fl">0.01</span>)
<span class="kw">predict</span>(lasso_object, X_new)</code></pre></div>
<p>Since there are efficient ways to cross-validate <span class="math inline">\(\lambda\)</span> for lasso regression, the package would likely also implement an interface like</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso_cv_object &lt;-<span class="st"> </span><span class="kw">lasso_cv</span>(X, y)
<span class="kw">predict</span>(lasso_cv_object)</code></pre></div>
<p>that would automatically select an optimal value of <span class="math inline">\(\lambda\)</span>. A nice package author would make <code>lasso</code> and <code>lasso_cv</code> generics and would also implement formula or even <a href="https://github.com/topepo/recipes">recipe</a> interfaces to the model, like so</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso_object &lt;-<span class="st"> </span><span class="kw">lasso</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> df, <span class="dt">lambda =</span> <span class="fl">0.01</span>)
lasso_cv_object &lt;-<span class="st"> </span><span class="kw">lasso_cv</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> df)</code></pre></div>
<p>I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because:</p>
<ul>
<li>When there isn’t a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with. That is, most of the time people work with multiple models, so it is incredibly convenient to be able to do something like:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_familys &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">lasso</span>(), <span class="kw">ridge</span>(), <span class="kw">OLS</span>())
train_models &lt;-<span class="st"> </span><span class="kw">map</span>(models, <span class="op">~</span><span class="kw">fit</span>(model_familys, y <span class="op">~</span><span class="st"> </span>., data))</code></pre></div>
<p>but this isn’t possible because each function has it’s own version of <code>fit</code>.</p>
<ul>
<li>Unless there’s a <code>recipe</code> interface to the <code>lasso_cv</code> function there isn’t a way to do principled preprocessing when resampling to estimate prediction error</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="objects-in-a-grammar-of-supervised-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="proposed-interface-for-supervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
