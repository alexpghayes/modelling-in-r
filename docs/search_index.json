[
["index.html", "Some thoughts on modelling in R Part 1 Motivation", " Some thoughts on modelling in R Alex Hayes 2017-12-15 Part 1 Motivation Each model in R essentially lives in its own package and has a unique interface. This introduces a large amount of cognitive load on data analysts. For example, suppose we want to use KNN. We might do something like this: library(tidyverse) library(rsample) data &lt;- initial_split(iris) train &lt;- training(data) test &lt;- select(testing(data), -Species) knn_preds &lt;- class::knn(test = test, train = select(train, -Species), cl = train$Species, k = 5) But if we want to use naive Bayes, we might end up writing code that looks like this: nb_model &lt;- e1071::naiveBayes(Species ~ ., data = train) nb_preds &lt;- predict(nb_model, newdata = test) This has some problems: knn generates predictions immediately on a test set, while naiveBayes creates a model object For knn we have to pass arguments cl and k even though it would be reasonable to select k by cross-validation and cl could be more succinctly expressed as an outcome in a formula knn and naiveBayes have different interfaces for specifying design matrices and outcomes knn and naiveBayes both return factor predictions by default, but this might not be the case for other packages. If we want to class probabilities, we have to pass prob = TRUE to knn and type = &quot;raw&quot; to predict.naiveBayes, and the outputs are in entirely different formats. That is, there isn’t a consistent interface to the packages themselves. Additionally, the packages don’t make use of a conceptual framework that makes it easy to think about modelling. The goal is the document is provide a grammar of modelling that resolves both of these problems. "],
["objects-in-a-grammar-of-supervised-learning.html", "Part 2 Objects in a grammar of supervised learning", " Part 2 Objects in a grammar of supervised learning When someone talks about supervised learning, they might say something like: “I fit K Nearest Neighbors on the dataset and got 57 percent accuracy.” This communicates the gist of what they did, but this isn’t enough information to reproduce what they actually did: we don’t know what value of \\(k\\) or the distance metric they used. When someone says that they used K Nearest Neighbors, they are referring to a family of supervised learning models, that is, KNN with \\(k \\in \\{1, 2, ...\\}\\) and some distance metric from a list of metrics, say, the \\(\\ell_p\\) metrics. Together all the possible values of \\(k\\) combined with all the \\(\\ell_p\\) metrics form a hyperparameter space, and we have to hope that our friend has selected hyperparameters in some reasonable way. Once our friend has selected hyperparameters, say \\(k=13\\) and Euclidean distance, they are now speaking about a specific, unambiguous model. Once someone gives us these details, we have enough information to reproduce the training process ourselves. These objects form the basis for our grammar: model family = modelling technique + hyperparameter space model = modelling technique + specific values of hyperparameters As a concrete example, consider the glmnet package. glmnet objects fit with a specific value of \\(\\lambda\\) correspond to model objects cv.glmnet objects correspond to a model family containing performance information for various hyperparameter values There are some important differences between model family objects and model objects. If we have some data, we know how to fit a model. With a model family that’s less obvious - we have to specify some hyperparameter selection method because it isn’t feasible to the train models for all possible valuables of the hyperparameters. Additionally, we typically perform inference on models rather than model familys. In practice what we’d like to do is specify a set of hyperparameters and train model objects for sets of hyperparameter combinations. Together this set of models will form the model family. Each of the models contained in the model family will have difference predictive performance, so an important part of the model family is a specification of a performance metric (i.e. RMSE) and a performance assess strategy (i.e. cross-validation). Keeping models and model familys in mind, let’s think about how existing machine learning libraries work. "],
["existing-approaches-to-supervised-learning.html", "Part 3 Existing approaches to supervised learning 3.1 Scikit-Learn 3.2 caret 3.3 mlr 3.4 Idiomatic modelling in R", " Part 3 Existing approaches to supervised learning 3.1 Scikit-Learn In Scikit-Learn we can fit and predict with model objects quite intuitively: from sklearn import neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) predictions = knn.predict(X_test) However, I find the abstractions to work with model familys less satisfying. Considering the KNN model family where we use random search to select a value of \\(k\\), which looks something like: hyperparameter_space = { &#39;n_neighbors&#39;: sp_randint(1, 31) # pick k in [1, 2, ..., 30] } knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=hyperparameter_space) knn.fit(X, y) I find this suboptimal for a couple reasons: We have to manually specify the hyperparameter space even though there are sane defaults If we wanted to use grid search or a different hyperparameter selection method, we’d have to change the specification of the hyperparameter space. That is, We’d have to pass a list of values for n_neighbors rather than a distribution function for a grid search. This doesn’t allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren’t doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select \\(k\\). Instead we recalculate pairwise distances for each \\(k\\), which is inefficient. In some cases sklearn provides work arounds to this, for example, with RIDGE regression: from sklearn import linear_model reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0]) reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None, normalize=False) Here we fit a RidgeCV object which efficiently performs cross-validation on the regularization parameter. However, now we have to remember to call RidgeCV rather than the standard grid search wrapper. It would be cleaner to specify that we want to work with the KNN/RIDGE model family and then plug in whichever hyperparameter selection technique we choose via the same interface. There are some things that Scikit-Learn does really well though: It has an extremely consistent interface that has been established as a communal standard within the Python community There are extension libraries that make automated machine learning and ensemble creation easy and pleasant Lastly, it’s worth noting that in Scikit-Learn you have to instantiate a KNeighborsClassifier object and afterward call fit on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single call, like so: knn_model &lt;- knn_classifer(y ~ ., data, k = 5) 3.2 caret In my mind, the caret library most closely matches my intuition about working with model familys rather than models. library(caret) # specify 10-fold CV repeated 10 times are hyperparameter selection strategy fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) knn_model &lt;- train(y ~ ., data = train_df, method = &quot;knn&quot;, trControl = fitControl) Some things I like about the caret interface: The hyperparameter selection strategy is an argument to the fit method Reasonable defaults are provided for some hyperparameters, so it occasionally does feel like you’re working with a model family that has abstracted away hyperparameter selection caret takes advantage of built in, smart hyperparameter selection like cv.glmnet instead of manually checking values of \\(\\lambda\\) Some things I don’t like about the caret interface: The default hyperparameter search is not an extensive enough to ignore hyperparameter selection in the majority of cases and so you end up specifying a tuning grid most of the time. Ideally I think all models should function with default hyperparameter selection as in h2o.automl() The result of train is an object of class train. That is, you know it’s a model family object, but the precise modelling technique is stored as a string. This means you have to write wrapper code if you want to add a model to the caret interface (i.e. it’s a nice package to work with, but it probably isn’t a great API to set as a communal standard for scientist producing packages) Ensembling is hard. There’s the caretEnsemble extension but I find the interface a messy. I find the names of several functions and function arguments unintuitive. It’s also worth noting that caret doesn’t require object instantiation like Scikit-Learn does. 3.3 mlr Briefly, the mlr library is similar to caret, but with the following interface: task = makeClassifTask(data = iris, target = &quot;Species&quot;) lrn = makeLearner(&quot;classif.lda&quot;) n = nrow(iris) train.set = sample(n, size = 2/3*n) test.set = setdiff(1:n, train.set) model = train(lrn, task, subset = train.set) pred = predict(model, task = task, subset = test.set) performance(pred, measures = list(mmce, acc)) I think this interface has issues similar to carets interface. Most importantly, however, I believe that the primary focus of supervised learning should be model familys, as opposed to tasks. I strongly believe the task at hand should be inferred from the class of the data object. I think this is important because we often think about inference in terms of models but almost never in terms of tasks. 3.4 Idiomatic modelling in R Let’s consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so library(lasso) lasso_object &lt;- lasso(X, y, lambda = 0.01) predict(lasso_object, X_new) Since there are efficient ways to cross-validate \\(\\lambda\\) for lasso regression, the package would likely also implement an interface like lasso_cv_object &lt;- lasso_cv(X, y) predict(lasso_cv_object) that would automatically select an optimal value of \\(\\lambda\\). A nice package author would make lasso and lasso_cv generics and would also implement formula or even recipe interfaces to the model, like so lasso_object &lt;- lasso(y ~ ., data = df, lambda = 0.01) lasso_cv_object &lt;- lasso_cv(y ~ ., data = df) I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because: When there isn’t a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with. That is, most of the time people work with multiple models, so it is incredibly convenient to be able to do something like: model_familys &lt;- list(lasso(), ridge(), OLS()) train_models &lt;- map(models, ~fit(model_familys, y ~ ., data)) but this isn’t possible because each function has it’s own version of fit. Unless there’s a recipe interface to the lasso_cv function there isn’t a way to do principled preprocessing when resampling to estimate prediction error "],
["proposed-interface-for-supervised-learning.html", "Part 4 Proposed interface for supervised learning 4.1 What we want out of a modelling interface 4.2 (Tentative) Proposal", " Part 4 Proposed interface for supervised learning Okay, now that we’ve taken a look at some interfaces and how they well they match up with the model/model family conception of modelling, let’s imagine an interface that makes operations on model and model familys feel natural in R. 4.1 What we want out of a modelling interface Work primarily with model family objects, where hyperparameter selection is abstracted as far away from the user as possible Scientists can provide smart cross-validation schemes when appropriate Users can easily select and interchange hyperparameter selection methods or specify their own Because hyperparameter / modelling technique specific settings are handled with reasonable defaults, users can easily work with large numbers of models are the same time via a consistent and unified interface Ensembling is easy Tidy and pipeable data structures 4.2 (Tentative) Proposal Since the Scikit-Learn interface is likely the most uniform and wide known interface, I think it’s a good idea to use language from Scikit-Learn as much as possible. # object of class &quot;knn&quot;. would like this to be cleaner knn &lt;- new_knn() # fit model with specific values of hyperparameters. class c(&quot;knn&quot;, &quot;model&quot;) knn_model &lt;- fit(knn, design, data, k = 13, metric = &quot;euclidean&quot;) # get hyperparameters via reasonable default. class c(&quot;knn&quot;, &quot;model_family&quot;) # design/data combo should work for: # - X, y (matrix, vector) # - formula, data frame # - recipe, data frame # # multiple dispatch would be nice here, curious how to implement # this assumes default arguments for hyperparameter selection that need # to be thought about more knn_family &lt;- fit(knn, design, data) # TO THINK ABOUT: not sure if hyperparameter definition being the sole # specification of model vs model_family output is a good idea # for convenience and to provide a standard interface, we define a wrapper # knn &lt;- fit(new_knn(), design, data). now a machine learning package has # an interface matching current idiomatic R for interested parties. but # new_* methods allow for convenient mapping of fit method, etc, that are # easiest when model instantiation and fitting are distinct knn_family &lt;- knn(design, data) # get the best knn model. class c(&quot;knn&quot;, &quot;model&quot;) best_knn_model &lt;- extract_model(knn_family) # predictions are of same type as outcome. i.e. numeric outcome # gives numeric predictions, factor outcome gives factor predictions # the following would behavior equivalently predictions &lt;- predict(knn_family, newdata) predictions &lt;- predict(best_knn_model, newdata) # for consistency with scikit-learn and overall sanity, for classification class_probs &lt;- predict_proba(knn_family, newdata) model_family fields: hp_results: tibble of hyperparameters and resulting performance hp_space: specification of hyperparameter space hp_strategy: object like trainControl to specify hyperparameter search best_model: best model from hyperparameter search once trained I don’t have good ideas on how to specify hp_space at the moment. I also think the following would highly increase usability bagged_model &lt;- bag(new_lasso(), new_ridge(), new_ols(), n = 50) stacked_model &lt;- stack(new_lasso(), new_ridge(), new_ols(), metalearner = new_glm()) boosted_model &lt;- boost(new_lasso(), new_ridge(), new_ols(), loss = &quot;some_loss&quot;) "],
["extension-to-unsupervised-learning.html", "Part 5 Extension to unsupervised learning 5.1 Pipelines 5.2 Non-repeatable mappings 5.3 Invertible Mappings", " Part 5 Extension to unsupervised learning This interface could also be extended to unsupervised learning following the spirit of Scikit-Learn by replacing the predict method with transform in the unsupervised case. I also think there’s an important equivalent of a model family in the unsupervised domain. For example, k-means and convex clustering both involve some sort of hyperparameter selection. For convex clustering you want to select a penalization parameter according to some strategy, probably optimizing one of several proposed clustering statistics For k-means you want to take the mode of cluster assignments, so predict(knn) is probably a bad idea, while predict(knn_family) is more reasonable in practice. I don’t know of any R packages that have the functionality you’d assume from predict(knn_family), I presume you always have to implement it yourself. I know h2o automatically selects k, but I’m not sure how the actual cluster assignments come about. I presume it’s reasonable though. 5.1 Pipelines Scikit-Learn offers pipelines that allow you to do things like do PCA on data and then fit a GBM to that data, but with an arbitrary number of steps. I’m curious how this should work with the current direction of modelling in R, where recipes would presumably take care of the PCA. What if you want to train a model on the GBM predictions? Do you need a new pipeline object, or do you make a step_gbm for a recipe? 5.2 Non-repeatable mappings It’s worth thinking about unsupervised techniques like t-SNE because you can transform data, but you can’t ever fit a t-SNE object because (non-parametric) t-SNE doesn’t define a mapping to a new space. 5.3 Invertible Mappings I would like a standard (S3 method?) for invertible mappings in the spirit of scale/unscale. In some sense recipes does this, but only for the forward transformation. scale/unscale store information about the transformation in the returned object, which I’m not sold on. I’d prefer something like: pca_model &lt;- pca(X) # recall pca(X) wraps fit(new_pca(), X) Z &lt;- transform(pca, X) # transform performs forward mapping X_recovered &lt;- untransform(pca, Z) # untransform performs inverse mapping # TODO: &quot;model&quot; probably isn&#39;t the best name for unsupervised transformations # even if those transformation do end up having &quot;model&quot;/&quot;model_family&quot; class Side note: I’d love to see a wrapper around PCA that easily lets the user specify a number of principal components to drop into irlba for high dimensional situations when computing the full PCA is overkill. "],
["utilities-and-other-important-functionality.html", "Part 6 Utilities and other important functionality", " Part 6 Utilities and other important functionality There are other important usability components that I think are being developed elsewhere: recipes package for specifying preprocessing pipeline yardstick package for tidy metrics tidyposter package for comparing trained models "],
["how-to-make-this-happen.html", "Part 7 How to make this happen 7.1 Roadblocks/needs before standardizing a modelling interface to community standardHow to implement new models 7.2 Infrastructure to provide 7.3 Misc", " Part 7 How to make this happen If the R community did accept these as communal standards for modelling, what steps would we need to take. 7.1 Roadblocks/needs before standardizing a modelling interface to community standardHow to implement new models I think it’s particular important to establish community standards for how to implement a recipes interface when you come up with a new model. How do you deal with intercepts for example? I’m hoping to make a PR to the recipes package with a detect_step function to simplify this, but I’m not sure if there are any recommendations on how or why to use this type of functionality. Related: a detailed tutorial for package developers describing how to conveniently deal with all three of the following data formats: X matrix, y vector/matrix response formula formula and data data frame recipe recipe and data data frame Similarly, I’d like to see rsample export an object describing a resampling scheme similar to fitControl so that researchers can implement consistent resampling interfaces across packages. For example, I’m currently helping out with a package that includes a large number of options specifying cross-validation controls that could all be wrapper together and made consistent across packages. (TODO: add lariat package details?). Question: Is an rset object sufficient specification? Should fit methods accept rset objects rather than data frames? Or is a specification type deal better because of likely resistance to rset objects? That, whic of the following is going to be more accepted: data data frame, fitControl equivalent specification rset data only. Does this imply an additional line of code going from the data to the rset? How does this influence workflow? Similarly, many modelling packages currently mix pre-processing and model fitting. Intercepts again come up as a sticky issue. If you want to be able to specify an intercept while providing both a recipe/data and X/y interface, you probably have to create a default fit(model, X, y, intercept) method and then a fit(model, recipe, data) that creates X and y and infers intercept. I find this unsatisfying and inelegant. The best solution seems to be making the recipe interface the standard, but I imagine a huge amount of push back on this (i.e. all my professors who say thinks like “I’ve never really gotten the point of data frames.”). Similarly, many regression packages provide arguments that all users to specify: if data should be centered if data should be scaled if some sort of dimension reduction should be applied to data how to deal with missing data I strongly believe the recipes package should handle all of the above. That way code can be more modular and consistent. 7.2 Infrastructure to provide Skeleton package defining general inference and explaining where to fill in the details for a particular modelling method Providing a system to check for interface compliance Provide an interface compliant machine learning library to demonstrate how nice the interface is 7.3 Misc How to incentivize busy profs to rewrite packages they no longer have time to think about? How far can GSOC and PR go? "]
]
