[
["index.html", "Some thoughts on modelling in R Chapter 1 Motivation", " Some thoughts on modelling in R Alex Hayes 2017-12-15 Chapter 1 Motivation Each model in R essentially lives in its own package and has a unique interface. This introduces a large amount of cognitive load on data analysts. For example, suppose we want to use KNN. We might do something like this: library(class) library(e1071) library(tidyverse) data(Boston, package = &quot;MASS&quot;) boston &lt;- as_tibble(Boston) boston train &lt;- boston %&gt;% slice(1:400) test &lt;- boston %&gt;% slice(401:506) %&gt;% select(-medv) knn_preds &lt;- knn(test = test, train = select(train, -medv), cl = train$medv, k = 5) But if we want to use naive Bayes, we might end up writing code that looks like this: nb_model &lt;- naiveBayes(medv ~ ., data = train) nb_preds &lt;- predict(nb_model, newdata = test, type = &quot;class&quot;) This has some problems: knn generates predictions immediately on a test set, while naiveBayes creates a model object knn and naiveBayes have different interfaces for specifying design matrices and outcomes for knn we have to pass arguments cl and k even though it would be reasonable to select k by cross-validation and cl could be more succinctly expressed as an outcome in a formula the predictions from naiveBayes object are not by default the same type as the outcome the user inputs That is, there isn’t a consistent interface to the packages themselves, and the packages don’t provide a conceptual framework that makes it easy to think about modelling. Before proposing an interface for modelling, I think it’s worthwhile to step back and define the objects that we’d like to work with and some reasonable actions we should be able to perform with those objects. "],
["objects-in-a-grammar-of-supervised-learning.html", "Chapter 2 Objects in a grammar of supervised learning", " Chapter 2 Objects in a grammar of supervised learning When someone talks about supervised learning, they might say something like: “I fit K Nearest Neighbors on the dataset and got 57 percent accuracy.” This communicates the gist of what they did, but this isn’t enough information to reproduce what they actually did: we don’t know what value of \\(k\\) or the distance metric they used. When someone says that they used K Nearest Neighbors, they are referring to a family of supervised learning models, that is, KNN with \\(k \\in \\{1, 2, ...\\}\\) and some distance metric from a list of metrics. Together all the possible values of \\(k\\) combined with all the possible metrics form a hyperparameter space, and we hope that our friend has selected hyperparameters in some reasonable way. Once our friend has selected hyperparameters, say \\(k=13\\) and Euclidean distance as a metric, they are now speaking about a specific, unambiguous model. Once someone gives us these details, we have enough information to reproduce the training process ourselves. These objects form the basis for our grammar: model family = modelling technique + hyperparameter space model = modelling technique + specific values of hyperparameters As a concrete example, consider the glmnet package. glmnet objects fit with a specific value of \\(\\lambda\\) correspond to model objects cv.glmnet objects correspond to model familys containing performance information for various hyperparameter values There are some important differences between model family objects and model objects. If we have some data, we know how to fit a model. With a model family that’s less obvious - we have to specify some hyperparameter selection method because it isn’t feasible to the train models for all possible valuables of the hyperparameters. Additionally, we typically perform inference on models rather than model familys. In practice what we’d like to do is specify a set of hyperparams and train model objects for sets of hyperparameter combinations. Together this set of models will form the model family. Each of the models contained in the model family will have difference predictive performance, so an important part of the model family is a specification of a performance metric (i.e. RMSE) and a performance assess strategy (i.e. cross-validation). Keeping models and model familys in mind, let’s think about how existing machine learning libraries work. "],
["existing-approaches-to-supervised-learning.html", "Chapter 3 Existing approaches to supervised learning 3.1 Scikit-Learn 3.2 caret 3.3 mlr 3.4 Idiomatic R code for single model", " Chapter 3 Existing approaches to supervised learning 3.1 Scikit-Learn In Scikit-Learn we can fit and predict with model objects quite intuitively: from sklearn import neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) predictions = knn.predict(X_test) However, I find the abstractions to work with model familys less satisfying. Considering the KNN model family where we use random search to select a value of \\(k\\), which looks something like: hyperparameter_space = { &#39;n_neighbors&#39;: sp_randint(1, 31) # pick k in [1, 2, ..., 30] } knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=hyperparameter_space) knn.fit(X, y) I find this suboptimal for a couple reasons: We have to manually specify the hyperparameter space even though there are sane defaults If we wanted to use grid search or a different hyperparameter selection method, we’d have to change the specification of the hyperparameter space (i.e. we’d have to pass a list of values for n_neighbors rather than a distribution function for a grid search, etc, etc) This doesn’t allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren’t doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select \\(k\\). Instead we recalculate pairwise distances for each \\(k\\), which is inefficient. Similarly, for penalized regression, we have to refit the model for each \\(\\lambda\\) rather than efficiently fitting an entire solution path all at once. That is, the KNN model family abstraction is closely tied to the hyperparameter selection technique, when it would be cleaner to specify that we want to work with the KNN model family and then plug in whichever hyperparameter selection technique we choose. There are some things that Scikit-Learn does really well though: It has an extremely consistent interface that has been established as a communal standard within the Python community There are extension libraries that make automated machine learning and ensemble creation easy and pleasant Lastly, it’s worth noting that in Scikit-Learn you have instantiate a KNeighborsClassifier object and afterward call fit on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single class along the lines of knn_model &lt;- knn_classifer(y ~ ., data, k = 5) 3.2 caret In my mind, the caret library most closely matches my intuition about working with model familys rather than models. library(caret) # specify 10-fold CV repeated 10 times are hyperparameter selection strategy fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) knn_model &lt;- train(y ~ ., data = train_df, method = &quot;knn&quot;, trControl = fitControl) Some things I like about the caret interface: The hyperparameter selection strategy is an argument to the fit method Reasonable defaults are provided for some hyperparameters, so it occasionally does feel like you’re working with a model family that has abstracted away hyperparameter selection caret takes advantage of built in, smart hyperparameter selection like cv.glmnet instead of manually checking values of \\(\\lambda\\) Some things I don’t like about the caret interface: The default hyperparameter search is not an extensive enough to ignore hyperparameter selection in the majority of cases and so you end up specifying a tuning grid most of the time. Ideally I think all models should function with default hyperparameter selection as in h2o.automl() The result of train is an object of class train. That is, you know it’s a model family object, but the precise modelling technique is stored as a string. This means you have to write wrapper code if you want to add a model to the caret interface (i.e. it’s a nice package to work with, but it probably isn’t a great API to set as a communal standard for scientist producing packages) Ensembling is hard. There’s the caretEnsemble extension but I find the interface a messy. It’s also worth noting that caret doesn’t require object instantiation like Scikit-Learn does. 3.3 mlr Briefly, the mlr library is similar to caret, but with the following interface: task = makeClassifTask(data = iris, target = &quot;Species&quot;) lrn = makeLearner(&quot;classif.lda&quot;) n = nrow(iris) train.set = sample(n, size = 2/3*n) test.set = setdiff(1:n, train.set) model = train(lrn, task, subset = train.set) pred = predict(model, task = task, subset = test.set) performance(pred, measures = list(mmce, acc)) I think this interface has issues similar to carets interface. In particular, I am strongly against manually specifying the learning task and think that the task at hand should be inferred from the class of the data object. I think prediction should be model-centric rather than outcome centric. 3.4 Idiomatic R code for single model Let’s consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so library(lasso) lasso_object &lt;- lasso(X, y, lambda = 0.01) predict(lasso_object, X_new) Since there are efficient ways to cross-validate \\(\\lambda\\) for lasso regression, the package would likely also implement an interface like lasso_cv_object &lt;- lasso_cv(X, y) predict(lasso_cv_object) that would automatically select an optimal value of \\(\\lambda\\). A nice package author would make lasso and lasso_cv generics and would also implement formula or even recipe interfaces to the model, like so lasso_object &lt;- lasso(y ~ ., data = df, lambda = 0.01) lasso_cv_object &lt;- lasso_cv(y ~ ., data = df) I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because: When there isn’t a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with Unless there’s a recipe interface to the lasso_cv function there isn’t a way to do principled preprocessing when resampling to estimate prediction error Most of the time you work with multiple models, so it is incredible convenient to be able to do something like: model_familys &lt;- list(lasso(), ridge(), OLS()) train_models &lt;- map(models, ~fit(model_familys, y ~ ., data)) but this isn’t possible because each function has it’s own version of fit. "],
["proposed-interface-for-supervised-learning.html", "Chapter 4 Proposed interface for supervised learning 4.1 Proposal (tentative start)", " Chapter 4 Proposed interface for supervised learning Okay, now that we’ve taken a look at some interfaces and how they well they match up with the model/model family conception of modelling, let’s imagine an interface that makes operations on model and model familys feel natural in R. Let’s recall what we want out of a modelling interface: Work primarily with model family objects, where hyperparameter selection is abstracted as far away from the user as possible Scientists can provide smart cross-validation schemes when appropriate Users can easily select and interchange hyperparameter selection methods or specify their own Because hyperparameter / modelling technique specific settings are handled with reasonable defaults, users can easily work with large numbers of models are the same time via a consistent and unified interface Ensembling is easy Tidy and pipeable data structures 4.1 Proposal (tentative start) Since the Scikit-Learn interface is likely the most uniform and wide known interface, I think it’s a good idea to use language from Scikit-Learn as much as possible. # object of class &quot;knn&quot;. would like this to be cleaner knn &lt;- new_knn() # fit model with specific values of hyperparameters. class c(&quot;knn&quot;, &quot;model&quot;) knn_model &lt;- fit(knn, design, data, k = 13, metric = &quot;euclidean&quot;) # get hyperparameters via reasonable default. class c(&quot;knn&quot;, &quot;model_family&quot;) # design/data combo should work for: # - X, y (matrix, vector) # - formula, data frame # - recipe, data frame # # multiple dispatch would be nice here, curious how to implement knn_family &lt;- fit(knn, design, data) # get the best knn model. class c(&quot;knn&quot;, &quot;model&quot;) best_knn_model &lt;- extract_model(knn_family) # predictions are of same type as outcome. i.e. numeric outcome # gives numeric predictions, factor outcome gives factor predictions # the following would behavior equivalently predictions &lt;- predict(knn_family, newdata) predictions &lt;- predict(best_knn_model, newdata) # for consistency with scikit-learn and overall sanity, for classification class_probs &lt;- predict_proba(knn_family, newdata) model_family fields: hp_results: tibble of hyperparameters and resulting performance hp_space: specification of hyperparameter space hp_strategy: object like trainControl to specify hyperparameter search best_model: best model from hyperparameter search once trained I don’t have good ideas on how to specify hp_space at the moment. I also think the following would highly increase usability bagged_model &lt;- bag(lasso(), ridge(), OLS(), n = 50) stacked_model &lt;- stack(lasso(), ridge(), OLS(), metalearner = GLM()) boosted_model &lt;- boost(lasso(), ridge(), OLS(), loss = &quot;some_loss&quot;) "],
["extension-to-unsupervised-learning.html", "Chapter 5 Extension to unsupervised learning 5.1 Pipelines?", " Chapter 5 Extension to unsupervised learning This interface could also be extended to unsupervised learning following the spirit of Scikit-Learn by replacing the predict method with transform in the unsupervised case. cv_model metalearner default grids 5.1 Pipelines? Could just create supervised learning steps 5.1.1 Special cases 5.1.1.1 Non-repeatable mappings 5.1.1.2 Invertible Mappings related to recipes but different it would nice to have a standard (S3 method?) for invertible mappings in the spirit of scale/unscale n some sense recipes does this, but only for the forward direction 1cale/unscale store information about the transform in the returned object, which i’m not sold on 1ut what about something like p# TODO: implement PCA example transformation, smart IRLBA/non-IRLBA selection supervised learning: models unsupervised learning: ??? transformation? clustering: cv equivalent to select number of clusters and average k-means assignments or penalty parameter for convex clustering pca_trans &lt;- pca(X) Z &lt;- forward(pca_trans, X) X_recovered &lt;- backward(pca_trans, Z) klearn does this with the fit/fit_transform methods but i don’t think provides a standard for the inverse mapping "],
["utilities-and-other-important-functionality.html", "Chapter 6 Utilities and other important functionality", " Chapter 6 Utilities and other important functionality There are other important usability components that I think are being developed elsewhere: recipes package for specifying preprocessing pipeline yardstick package for tidy metrics tidyposter package for comparing trained models "],
["how-to-make-this-happen.html", "Chapter 7 How to make this happen 7.1 How to implement new models 7.2 Wrapping appropriately implemented models together into a machine learning library", " Chapter 7 How to make this happen If the R community did accept these as communal standards for modelling, what steps would we need to take. 7.1 How to implement new models (provide a skeleton package containing default methods to fill out) 7.2 Wrapping appropriately implemented models together into a machine learning library How to incentivize busy profs to rewrite packages they no longer have time to think about? "]
]
