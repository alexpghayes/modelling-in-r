[
["index.html", "Some thoughts on modelling in R Part 1 Motivation", " Some thoughts on modelling in R Alex Hayes 2017-12-19 Part 1 Motivation Each model in R essentially lives in its own package and has a unique interface. This introduces a large amount of cognitive load on data analysts. For example, suppose we want to use KNN. We might do something like this: library(tidyverse) library(rsample) data &lt;- initial_split(iris) train &lt;- training(data) test &lt;- select(testing(data), -Species) knn_preds &lt;- class::knn(test = test, train = select(train, -Species), cl = train$Species, k = 5) But if we want to use naive Bayes, we might end up writing code that looks like this: nb_model &lt;- e1071::naiveBayes(Species ~ ., data = train) nb_preds &lt;- predict(nb_model, newdata = test) This has some problems: knn generates predictions immediately on a test set, while naiveBayes creates a model object For knn we have to pass arguments cl and k even though it would be reasonable to select k by cross-validation and cl could be more succinctly expressed as an outcome in a formula knn and naiveBayes have different interfaces for specifying design matrices and outcomes knn and naiveBayes both return factor predictions by default, but this might not be the case for other packages. If we want to class probabilities, we have to pass prob = TRUE to knn and type = &quot;raw&quot; to predict.naiveBayes, and the outputs are in entirely different formats. That is, there isn’t a consistent interface to the packages themselves. Additionally, the packages don’t make use of a conceptual framework that makes it easy to think about modelling. The goal is the document is provide a grammar of modelling that resolves both of these problems. "],
["objects-in-a-grammar-of-supervised-learning.html", "Part 2 Objects in a grammar of supervised learning", " Part 2 Objects in a grammar of supervised learning When someone talks about supervised learning, they might say something like: “I fit K Nearest Neighbors on the dataset and got 57 percent accuracy.” This communicates the gist of what they did, but this isn’t enough information to reproduce what they actually did: we don’t know what value of \\(k\\) or the distance metric they used. When someone says that they used K Nearest Neighbors, they are referring to a family of supervised learning models, that is, KNN with \\(k \\in \\{1, 2, ...\\}\\) and some distance metric from a list of metrics, say, the \\(\\ell_p\\) metrics. Together all the possible values of \\(k\\) combined with all the \\(\\ell_p\\) metrics form a hyperparameter space, and we have to hope that our friend has selected hyperparameters in some reasonable way. Once our friend has selected hyperparameters, say \\(k=13\\) and Euclidean distance, they are now speaking about a specific, unambiguous model. Once someone gives us these details, we have enough information to reproduce the training process ourselves. These objects form the basis for our grammar: model family = modelling technique + hyperparameter space model = modelling technique + specific values of hyperparameters As a concrete example, consider the glmnet package. glmnet objects fit with a specific value of \\(\\lambda\\) correspond to model objects cv.glmnet objects correspond to a model family containing performance information for various hyperparameter values There are some important differences between model family objects and model objects. If we have some data, we know how to fit a model. With a model family that’s less obvious - we have to specify some hyperparameter selection method because it isn’t feasible to the train models for all possible valuables of the hyperparameters. Additionally, we typically perform inference on models rather than model familys. In practice what we’d like to do is specify a hyperparameter space train model objects for certain hyperparameter combinations. The resultant set of models forms a sort of empirical model family. Keeping models and model familys in mind, let’s think about how existing machine learning libraries work. "],
["existing-approaches-to-supervised-learning.html", "Part 3 Existing approaches to supervised learning 3.1 Scikit-Learn 3.2 caret 3.3 mlr 3.4 Idiomatic modelling in R", " Part 3 Existing approaches to supervised learning 3.1 Scikit-Learn In Scikit-Learn we can fit and predict with model objects quite intuitively: from sklearn import neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) predictions = knn.predict(X_test) However, I find the abstractions to work with model familys less satisfying. Considering the KNN model family where we use random search to select a value of \\(k\\), which looks something like: hyperparameter_space = { &#39;n_neighbors&#39;: sp_randint(1, 31) # pick k in [1, 2, ..., 30] } knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=hyperparameter_space) knn.fit(X, y) I find this suboptimal for a couple reasons: We have to manually specify the hyperparameter space even though there are sane defaults If we wanted to use grid search or a different hyperparameter selection method, we’d have to change the specification of the hyperparameter space. That is, We’d have to pass a list of values for n_neighbors rather than a distribution function for a grid search. This doesn’t allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren’t doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select \\(k\\). Instead we recalculate pairwise distances for each \\(k\\), which is inefficient. In some cases sklearn provides work arounds to this, for example, with RIDGE regression: from sklearn import linear_model reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0]) reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None, normalize=False) Here we fit a RidgeCV object which efficiently performs cross-validation on the regularization parameter. However, now we have to remember to call RidgeCV rather than the standard grid search wrapper. It would be cleaner to specify that we want to work with the KNN/RIDGE model family and then plug in whichever hyperparameter selection technique we choose via the same interface. There are some things that Scikit-Learn does really well though: It has an extremely consistent interface that has been established as a communal standard within the Python community There are extension libraries that make automated machine learning and ensemble creation easy and pleasant Lastly, it’s worth noting that in Scikit-Learn you have to instantiate a KNeighborsClassifier object and afterward call fit on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single call, like so: knn_model &lt;- knn_classifer(y ~ ., data, k = 5) 3.2 caret In my mind, the caret library most closely matches my intuition about working with model familys rather than models. library(caret) # specify 10-fold CV repeated 10 times as resampling strategy fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) # uses grid search on k = [5, 7, 9] knn_model &lt;- train(Species ~ ., data = iris, method = &quot;knn&quot;, trControl = fitControl) Some things I like about the caret interface: The hyperparameter selection strategy is an argument to the fit method Reasonable defaults are provided for some hyperparameters, so it occasionally does feel like you’re working with a model family that has abstracted away hyperparameter selection caret takes advantage of built in, smart hyperparameter selection like cv.glmnet instead of manually checking values of \\(\\lambda\\) Some reasons why caret isn’t a complete solution in my mind: The default hyperparameter search is not an extensive enough to ignore hyperparameter selection in the majority of cases and so you end up specifying a tuning grid most of the time. Ideally I think all models should function with default hyperparameter selection as in h2o.automl() It’s a nice package to work with, but it probably isn’t an API to set as a communal standard for scientists producing packages. Ensembling is hard. There’s the caretEnsemble extension but I find the interface somewhat hard to use. In particular, it seems like there’s a lot of work on using influence functions/TMLE to do targeted inference with ML at the moment, and many of the packages coming out make use of the SuperLearner package, when I think a SuperLearner should just be one of several (consistently designed) ensemble objects. It’s also worth noting that caret doesn’t require object instantiation like Scikit-Learn does. 3.3 mlr Briefly, the mlr library is similar to caret, but with the following interface: task = makeClassifTask(data = iris, target = &quot;Species&quot;) lrn = makeLearner(&quot;classif.lda&quot;) n = nrow(iris) train.set = sample(n, size = 2/3*n) test.set = setdiff(1:n, train.set) model = train(lrn, task, subset = train.set) pred = predict(model, task = task, subset = test.set) performance(pred, measures = list(mmce, acc)) I think this interface has issues similar to carets interface. Most importantly, however, I believe that the primary focus of supervised learning should be model familys, as opposed to tasks. I strongly believe the task at hand should be inferred from the class of the data object. I think this is important because we often think about inference in terms of models but almost never in terms of tasks. 3.4 Idiomatic modelling in R Let’s consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so library(lasso) lasso_object &lt;- lasso(X, y, lambda = 0.01) predict(lasso_object, X_new) Since there are efficient ways to cross-validate \\(\\lambda\\) for lasso regression, the package would likely also implement an interface like lasso_cv_object &lt;- lasso_cv(X, y) predict(lasso_cv_object) that would automatically select an optimal value of \\(\\lambda\\). A nice package author would make lasso and lasso_cv generics and would also implement formula or even recipe interfaces to the model, like so lasso_object &lt;- lasso(y ~ ., data = df, lambda = 0.01) lasso_cv_object &lt;- lasso_cv(y ~ ., data = df) I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because: When there isn’t a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with. That is, most of the time people work with multiple models, so it is incredibly convenient to be able to do something like: model_familys &lt;- list(lasso(), ridge(), OLS()) train_models &lt;- map(models, ~fit(model_familys, y ~ ., data)) but this isn’t possible because each function has it’s own version of fit. Unless there’s a recipe interface to the lasso_cv function there isn’t a way to do principled preprocessing when resampling to estimate prediction error "],
["proposed-interface-for-supervised-learning.html", "Part 4 Proposed interface for supervised learning 4.1 What we want out of a modelling interface 4.2 Model Instantiation 4.3 Model fitting 4.4 Prediction 4.5 Ensembling", " Part 4 Proposed interface for supervised learning Okay, now that we’ve taken a look at some interfaces and how they well they match up with the model/model family conception of modelling, let’s imagine an interface that makes operations on model and model familys feel natural in R. 4.1 What we want out of a modelling interface Scientists can provide smart cross-validation schemes when appropriate Users can easily select and interchange hyperparameter selection methods or specify their own Because hyperparameter / modelling technique specific settings are handled with reasonable defaults, users can easily work with large numbers of models are the same time via a consistent and unified interface Ensembling is easy (in particular, I think we want it to be easy to build SuperLearner based packages off the provided model objects) Tidy and pipeable data structures Work primarily with model family objects, where hyperparameter selection is abstracted as far away from the user as possible. The less time users have to spend writing the same old hyperparameter optimization code, the easier it is for them to fall into the pit of success. Provide metalearned hyperparameter values like auto-sklearn does (i.e. default hyperparameter search starts with hyperparameters known to work well on similar datasets) Since the Scikit-Learn interface is likely the most uniform and wide known interface, I think it’s a good idea to use language from Scikit-Learn as much as possible. 4.2 Model Instantiation Let’s demonstrate a potential interface assuming we’d like to use KNN. In terms of implementation, I think things will be easiest if each model has a dedicated object initialization function. This function should return an object of class &quot;knn&quot;. knn &lt;- new_knn() # instantiation functions need a better name But since the current paradigm in R doesn’t involve instantiating model objects before fitting them, I think it would also be good to provide a wrapper called knn that first creates a knn object and then fits it. That is knn_family &lt;- knn(design, data) would be equivalent to knn &lt;- new_knn() knn_family &lt;- fit(knn, design, data) # or, with pipes knn_family &lt;- new_knn() %&gt;% fit(design, data) where design/data is some combination of: X, y (matrix, vector) formula, data frame recipe, data frame 4.3 Model fitting Model fitting is the most important and most complex part of the interface, and this is where I’m least happy with what’s I’ve come up with at the moment. To fit a model, we need to specify: A design matrix and data A hyperparameter space A hyperparameter search algorithm A resampling/performance assessment strategy I believe each of these is a major component that deserves it’s own object. The recipes package deals with specifying data design. There are several R packages that allow for specification of a hyperparameter space, such as mlr/mlrMBO (details here). The tools there provide seem quite useful, but again I’m not a fan of the interface. There are a huge number of packages that do this in Python though, for example Spearmint. For now, I’ll pretend hp_space() generates a reasonable hyperparameter space object, and that gaussian_process_opt() is a search strategy that knows how to interface with hp_space objects. I’ll also assume that there is an rsample_spec object that specifies a resampling strategy and some performance metric used to determine which model has the best performance. Maybe this would turn out to be a combination of an rsample::rset generator with a scoring function. To fit a model object, we could then do any of the following, returning an object of class knn_model knn_model &lt;- knn(design, data, hp_space(k = 13, metric = &quot;euclidean&quot;)) knn_model &lt;- fit(new_knn(), design, data, hp_space = hp_space(k = 13, metric = &quot;euclidean&quot;)) knn_model &lt;- new_knn() %&gt;% add_design(recipe, data) %&gt;% add_hp_space(k = 13, metric = &quot;euclidean&quot;) %&gt;% fit() Since we are fitting a model rather than model_family here we don’t need to specify a hyperparameter search algorithm or a performance assessment specification. That is, you get a model back when there is a single set of hyperparameters in the hp_space and a model_family anytime the hp_space specifies multiple/infinite hyperparameter combinations. To fit model_family objects, the following would be equivalent knn_family &lt;- knn(design, data) # and showing default arguments knn_family &lt;- fit(new_knn(), design, data, hp_space = default_knn_hp_space, hp_strategy = gaussian_process_opt, resampling_strategy = default_rset_specification) For users departing from the defaults, this might look like hyperparams &lt;- hp_space(k = 3:4, metric = c(&quot;euclidean&quot;, &quot;manhattan&quot;)) resamp_spec &lt;- resampling_spec(score = &quot;mae&quot;, sampling = &quot;bootstrap&quot;, reps = 10) knn_family &lt;- new_knn() %&gt;% add_design(recipe, data) %&gt;% add_hp_space(hyperparams) %&gt;% add_hp_search(hyperband) %&gt;% add_resampling(resamp_spec) %&gt;% fit() If you wanted to do inference on the best model in knn_family, you could get it with best_knn_model &lt;- extract_model(knn_family) This still leaves out a bunch of details. For example (I’ll update the list below as I think of more things): Observation weights and offsets. My thought is that recipes should handle this. A subset of the data to work on. Again, recipes should handle this to separate modelling fitting and data preprocessing. 4.4 Prediction Default predict methods should always return predictions of the same type as the input data. That is, if you specify a numeric outcome, you get a numeric prediction, if you specify a factor outcome, you get a factor prediction. This makes it easy for users to assess model performance, which is probably the first thing you want to do do after predicting. This would look like predictions &lt;- predict(knn_family, newdata) predictions &lt;- predict(best_knn_model, newdata) For sanity and consistency with Scikit-Learn, I think it would be good to add a new generic predict_proba to get class probabilities for classification problems class_probs &lt;- predict_proba(knn_family, newdata) 4.5 Ensembling I also think the following would highly increase usability bagged_model &lt;- bag(new_lasso(), new_ridge(), new_ols(), n = 50) stacked_model &lt;- stack(new_lasso(), new_ridge(), new_ols(), metalearner = new_glm()) boosted_model &lt;- boost(new_lasso(), new_ridge(), new_ols(), loss = &quot;some_loss&quot;) I’m not sure if bagged_model, stacked_model and boosted_model are models or model_familys, or something else entirely. "],
["extension-to-unsupervised-learning.html", "Part 5 Extension to unsupervised learning 5.1 Pipelines 5.2 Non-repeatable mappings 5.3 Invertible Mappings", " Part 5 Extension to unsupervised learning This interface could also be extended to unsupervised learning following the spirit of Scikit-Learn by replacing the predict method with transform in the unsupervised case. I also think there’s an important equivalent of a model family in the unsupervised domain. For example, k-means and convex clustering both involve some sort of hyperparameter selection. For convex clustering you want to select a penalization parameter according to some strategy, probably optimizing one of several proposed clustering statistics For k-means you want to take the mode of cluster assignments, so predict(knn) is probably a bad idea, while predict(knn_family) is more reasonable in practice. I don’t know of any R packages that have the functionality you’d assume from predict(knn_family), I presume you always have to implement it yourself. I know h2o automatically selects k, but I’m not sure how the actual cluster assignments come about. I presume it’s reasonable though. 5.1 Pipelines Scikit-Learn offers pipelines that allow you to do things like do PCA on data and then fit a GBM to that data, but with an arbitrary number of steps. I’m curious how this should work with the current direction of modelling in R, where recipes would presumably take care of the PCA. What if you want to train a model on the GBM predictions? Do you need a new pipeline object, or do you make a step_gbm for a recipe? 5.2 Non-repeatable mappings It’s worth thinking about unsupervised techniques like t-SNE because you can transform data, but you can’t ever fit a t-SNE object because (non-parametric) t-SNE doesn’t define a mapping to a new space. 5.3 Invertible Mappings I would like a standard (S3 method?) for invertible mappings in the spirit of scale/unscale. In some sense recipes does this, but only for the forward transformation. scale/unscale store information about the transformation in the returned object, which I’m not sold on. I’d prefer something like: pca_model &lt;- pca(X) # recall pca(X) wraps fit(new_pca(), X) Z &lt;- transform(pca, X) # transform performs forward mapping X_recovered &lt;- untransform(pca, Z) # untransform performs inverse mapping # TODO: &quot;model&quot; probably isn&#39;t the best name for unsupervised transformations # even if those transformation do end up having &quot;model&quot;/&quot;model_family&quot; class Side note: I’d love to see a wrapper around PCA that easily lets the user specify a number of principal components to drop into irlba for high dimensional situations when computing the full PCA is overkill. "],
["how-to-make-this-happen.html", "Part 6 How to make this happen 6.1 Community standards 6.2 Infrastructure to provide 6.3 Misc", " Part 6 How to make this happen If the R community did accept these as communal standards for modelling, what steps would we need to take? 6.1 Community standards I think it’s particular important to establish community standards for how to implement a recipes interface when you come up with a new model. How do you deal with intercepts for example? I’m hoping to make a PR to the recipes package with a detect_step function to simplify this, but I’m not sure if there are any recommendations on how or why to use this type of functionality. Related: a detailed tutorial for package developers describing how to conveniently deal with all three of the following data formats: X matrix, y vector/matrix response formula formula and data data frame recipe recipe and data data frame Similarly, I’d like to see rsample export an object describing a resampling scheme similar to fitControl so that researchers can implement consistent resampling interfaces across packages. For example, I’m currently helping out with a package that includes a large number of options specifying cross-validation controls that could all be wrapper together and made consistent across packages. (TODO: add lariat package details?). Question: Is an rset object sufficient specification? Should fit methods accept rset objects rather than data frames? Or is a specification type deal better because of likely resistance to rset objects? That, whic of the following is going to be more accepted: data data frame, fitControl equivalent specification rset data only. Does this imply an additional line of code going from the data to the rset? How does this influence workflow? Similarly, many modelling packages currently mix pre-processing and model fitting. Intercepts again come up as a sticky issue. If you want to be able to specify an intercept while providing both a recipe/data and X/y interface, you probably have to create a default fit(model, X, y, intercept) method and then a fit(model, recipe, data) that creates X and y and infers intercept. I find this unsatisfying and inelegant. The best solution seems to be making the recipe interface the standard, but I imagine a huge amount of push back on this (i.e. all my professors who say thinks like “I’ve never really gotten the point of data frames.”). Similarly, many regression packages provide arguments that all users to specify: if data should be centered if data should be scaled if some sort of dimension reduction should be applied to data how to deal with missing data I strongly believe the recipes package should handle all of the above. That way code can be more modular and consistent. 6.2 Infrastructure to provide Skeleton package defining general inference and explaining where to fill in the details for a particular modelling method Providing a system to check for interface compliance Provide an interface compliant machine learning library to demonstrate how nice the interface is 6.3 Misc How to incentivize busy profs to rewrite packages they no longer have time to think about? How far can GSOC and PRs go? "]
]
