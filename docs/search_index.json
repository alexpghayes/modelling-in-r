[
["index.html", "Some thoughts on modelling in R Part 1 Motivation", " Some thoughts on modelling in R Alex Hayes 2018-04-10 Part 1 Motivation Each model in R essentially lives in its own package and has a unique interface. This introduces a large amount of cognitive load on data analysts. For example, suppose we want to use KNN. We might do something like this: library(tidyverse) library(rsample) data &lt;- initial_split(iris) train &lt;- training(data) test &lt;- select(testing(data), -Species) knn_preds &lt;- class::knn(test = test, train = select(train, -Species), cl = train$Species, k = 5) But if we want to use naive Bayes, we might end up writing code that looks like this: nb_model &lt;- e1071::naiveBayes(Species ~ ., data = train) nb_preds &lt;- predict(nb_model, newdata = test) This has some problems: knn generates predictions immediately on a test set, while naiveBayes creates a model object For knn we have to pass arguments cl and k even though it would be reasonable to select k by cross-validation and cl could be more succinctly expressed as an outcome in a formula knn and naiveBayes have different interfaces for specifying design matrices and outcomes knn and naiveBayes both return factor predictions by default, but this might not be the case for other packages. If we want to class probabilities, we have to pass prob = TRUE to knn and type = &quot;raw&quot; to predict.naiveBayes, and the outputs are in entirely different formats. That is, there isn’t a consistent interface to the packages themselves. Additionally, the packages don’t make use of a conceptual framework that makes it easy to think about modelling. The goal of the document is provide a grammar of modelling that resolves both of these problems. "],
["models-and-model-families.html", "Part 2 Models and model families", " Part 2 Models and model families When someone talks about supervised learning, they might say something like: “I fit K Nearest Neighbors on the dataset and got 57 percent accuracy.” This communicates the gist of what they did, but this isn’t enough information to reproduce what they actually did: we don’t know what value of \\(k\\) or the distance metric they used. When someone says that they used K Nearest Neighbors, they are referring to a family of supervised learning models, that is, KNN with \\(k \\in \\{1, 2, ...\\}\\) and some distance metric from a list of metrics, say, the \\(\\ell_p\\) metrics. Together all the possible values of \\(k\\) combined with all the \\(\\ell_p\\) metrics form a hyperparameter space, and we have to hope that our friend has selected hyperparameters in some reasonable way. Once our friend has selected hyperparameters, say \\(k=13\\) and Euclidean distance, they are now speaking about a specific, unambiguous model. With these hyperparameters, we have enough information to fit exactly the same model ourselves. These objects form the basis for our grammar: model family = modelling technique + hyperparameter space model = modelling technique + specific values of hyperparameters As a concrete example, consider the glmnet package. glmnet objects fit with a specific value of \\(\\lambda\\) correspond to model objects cv.glmnet objects correspond to a model family containing performance information for various hyperparameter values Once fit, a model family is a set of fit models. There are important differences between model family objects and model objects. For example, we typically perform inference on models rather than model families. "],
["the-modelling-process.html", "Part 3 The modelling process", " Part 3 The modelling process People working with data create statistical models for three major reasons: To describe data and perform exploratory data analysis To estimate the (causal) effect of a treatment on a response To develop a predictive model for some response of interest A model is often the best descriptive statistic. -Frank Harrell on Twitter https://twitter.com/f2harrell/status/983675033200549888 Exploratory data analysis might consist of: using unsupervised learning techniques to find patterns using supervised learning to look for patterns ask and answer many rapidly formed causal / associational questions in quick succession succession sharing these findings with others in technical or non-technical reports (Causal) inference might consist of: following a preregistered analysis plan to use a several well specified models to the data Diagnostics to make sure modelling assumptions are satisfied This is in theory straightforward if you’ve already done the conceptual work to figure out what should go in your model and what type of model you should fit, what the coefficents will mean, etc, etc. But in a more exploratory analysis you might brainstorm and try several different models to see which are appropriate for the data, and also you might need to select one of several potentially appropriate models. Developing a predictive model might consist of: Looking at the data and seeing what kinds of features you can engineer Fitting a metric ton of models, some particularly smart ones suggested by looking at the data Assessing the performance of each of the models Comparing the models to see which has best performance Making sure that complex models have converged (MCMC or gradient descent) Probing each model with stuff like LIME to understand it and see avenues for improvement Combining models into a well-tested ensemble for use in production when the inputs can become all kinds of nonsense Once our data scientist friend has data, the big picture is that they will: Spend most of their time cleaning and tidying the data to get it into a bare minimum usable format. Brainstorm up appropriate models and model families to provide solutions for their inferential, predictive and exploratory tasks. Fit a variety, perhaps many thousands, or these models and model families to the data, which wanting to compare and iterator through the various Diagnose model fit, check model convergence, compare model performance, visualize model coefficients, predict on new data, etc. Look at model parameters. Compare model statistics for many models. Assess a model for many values of a hyperparameter. Somehow turn this into a meaningful piece of information, business product, or a report to share with other people. The goal is to focus on steps (3) and (4) in the hopes that coherent conceptual frame work will emerge. Whether the key aspect of this framwork is “tidyness” or something else that I think remains to be seen. Next we discuss some general properties that will make a modelling library desirable, both in terms of conceptual organization and practical considerations. "],
["interface-desirables.html", "Part 4 Interface desirables 4.1 Desirables necessary for conceptual clarity 4.2 Desirables necessary for practicality", " Part 4 Interface desirables 4.1 Desirables necessary for conceptual clarity Each model and model family corresponds to a single distinct class This makes it easy to provide an interface compliant template that researchers can will fill in with new models they implement. It also allows them to write custom S3 methods, which they likely expect, and to provide smart cross-validation schemes when appropriate. Type safe in/out definitions for each method on each class Users think in terms of model familys Since it’s most intuitive to think about model families, we want the interface to reflect this. For operations that require an actual model, such as prediction, users should be able to pass in model family objects, and then have the software automatically extract and use the best model in the model family. The interface should provide a clear idea of what it means to fit a model family, so that interested users can customize hyperparameter spaces and selection techniques. 4.2 Desirables necessary for practicality Uses as much language from Scikit-Learn as possible Since Scikit-Learn is likely the most uniform and widely known interface for predictive modelling, this reduces cognitive load. Additionally, the Python ML world has a strong tradition of implementing new models with the Scikit-Learn interface that we hope will carry over to new researchers implementing new methods in R. Researchers can easily implement new models that conform to the interface This is because we often want to use multiple models at the same time, and it’s a nightmare when these models don’t share the same API. Ensembling is easy Most users interested in prediction will create an ensemble at some point. To appeal to the Kaggle crowd as much as possible, it should be easy to bag, boost and stack models to arbitrary depths. Additionally, a number of R packages use the SuperLearner package as the basis for Super Learner and semi-parametric inference. If new machine learning techniques implement a standardized interface, then packages like SuperLearner (and also caret and mlr for that matter) don’t need to implement wrappers for each new model. Facilitates automated machine learning Recently industrial data science has been tending toward automating as many redundant parts of the modelling process as possible. Take for example: H2O’s Automated Machine Learning, which has already landed in R with h2o.automl() H2O’s Driverless AI Airbnb’s modelling workflow TPOT mlrMBO Spearmint In terms of implementation, this means it should be easy to programmatically generate and manipulate model and model familys. This also means that hyperparameter space specifications should be flexible and informative enough to handle new hyperparameter search algorithms, and that it should be easy to specify smart defaults. I think it would also be nice to “metalearn” good hyperparameters to try initially, as in auto-sklearn. Tidy and pipeable data structures The interface should conform to tidyverse principles, given the consistent standards and wide user adoption of the tidyverse. "],
["proposed-interface-model-fitting.html", "Part 5 Proposed interface: model fitting 5.1 Fitting a model object 5.2 Objects in play 5.3 Model Instantiation 5.4 Model fitting 5.5 Prediction 5.6 Shortcut methods", " Part 5 Proposed interface: model fitting Let’s imagine an interface that makes operations on model and model familys feel natural in R. We begin by discussing the process of fitting models and model families, primarily from the perspective of predictive models. In some sense, you can work with a model by fitting a model family on a hyperparameter grid containing a single point. This is the approach caret takes, and I believe the one present in the current interface proposal. I think is minorly problematic in terms of conceptual clarity, but majorly problematic in terms of implementation of new models. If you’re implementing a new modelling technique, it makes a lot more sense to first write a fit method for models (i.e. glmnet::glmnet) and then to write a fit method (i.e. hyperparameter selection method) that may make heavy use of fit.model. I want this separation because I think it’ll be key to selling an interface to people writing new methods 5.1 Fitting a model object getting to data: data preprocessing: some of this is going to be model based data augmentation, filtering and variable selection, up/downsampling, unsupervised transformations, etc To fit a model, we need: Data Specific hyperparameter values A way to train the model given data and specific hyperparameters On the other hand, to fit a model family, we need: getting to data: data preprocessing: some of this is going to be model based data augmentation, filtering and variable selection, up/downsampling, unsupervised transformations, etc use data to estimate certain values (maybe hyperparameter values) to input into a model - use data scale to define a hyperparameter search range, for example Data The hyperparameter space to consider A way to train the model given data and specific hyperparameters A way to search through hyperparameter space A way to determine which trained model is best To find the best model in the model family, we need a performance metric, such as root mean squared error, and an estimate of this metric on out-of-sample model data. This likely means getting multiple estimates of model performance by fitting the same model on resampled datasets. model calibration: specifies both what you need to compare models and how to compare models: think about: compatible types of resampling: different types of CV that you could safely use together We first need to specify all of these in order to fit a model. 5.2 Objects in play Recall that to fit a model, we need: Data Specific hyperparameter values A way to train the model given data and specific hyperparameters On the other hand, to fit a model family, we need: Data The hyperparameter space to consider A way to train the model given data and specific hyperparameters A way to search through hyperparameter space A way to determine which trained model is best Our task is now to design intuitive ways to specify all of these. Thankfully, Max Kuhn has already solved several of these problems for us: The recipes package creates maps from messy input data to design matrices, generalizing the formula. The learned map can then be applied to new data. The yardstick package provides tidy calculations of various performance metrics given predictions and the baseline truth The tidyposterior package provides methods to compare models within a model family by comparing resampled performance metrics The rsample package provides infrastructure for a variety of resampling strategies (although does not provide a way to specify a resampling scheme beyond raw rsets) CRAN provides packages to fit most models of interest. This leaves us with a couple remaining problems that we will assume have nice solutions for the moment. 5.2.1 Model calibration We need to be able to find the best model in a given model family. For the sake of this document, we’ll assume there’s an imaginary calibration object that consists of: (1) a resampling specification, (2) a performance metric and (3) an appropriate strategy for comparing performance metrics. As a concrete example, a calibration object might specify that each model in the model should be fit on 20 bootstrap samples, and the best model would have the lowest average training RMSE on resamples datasets. TODO: min RMSE, or min RMSE within 1-SE following Breiman 5.2.2 Hyperparameter space definition To my knowledge, the various hyperparameter search methods use hyperparameter spaces defined as: - Probability distributions over HP space (random search algorithms) - Fixed sets of points in HP space (grid search, possibly Gaussian procress or tree Parzen estimators as an initial grid) GP/TPE could also use a probability distribution initially, with some smart initial sampling scheme to pick initial points. This is what mlrMBO does. auto-sklearn provides an initial grid for GP/TPE based on hyperparameter values that work well on a library of previous datasets and calls the approach “metalearning.” So presumable we want hp_dist and hp_grid objects that both subclass hp_space objects. We could even provide semi-sane translation between the two. hp_grid_to_dist would guess the domain of the hyperparameters hp_dist_to_grid could sample at quantiles or on a latin space design or whatever is smartest To specify hp_dist objects we should look at Hyperopt specifications. Doing things on log scale will probably be important, and we should think about important transformations for hyperparameters and how to handle them. More broadly, the model/model family framework can extend beyond supervised learning. For k-means, you might want a fit.k_means_family to select k according to some reasonable strategy. Just something to keep in mind. For now, I’m going to assume that the problem of hyperparameter space definition has been solved, and that there are nice hp_space objects that contain this information. 5.2.3 Hyperparameter search Similarly, let’s assume that there are standard functions for searching through hyperparameter space of class hp_search. 5.2.4 Model and model family objects 5.2.4.1 model trained: a logical indicating if the model has been fit design: a recipe specifying a transformation into a design matrix hyperparameters: a named list of hyperparameters to fit the model 5.2.4.2 model_family trained: a logical indicating if the model has been fit design: a recipe specifying a transformation into a design matrix hp_space: a hp_space object hp_search: a hp_search object calibration: a calibration object 5.2.4.3 Pipeable helpers Each of the following would accept a model or model_family object and update the appropriate field: add_design would return a model with an updated design field. It would nice for this to be a generic that also had matrix and formula methods that promoted data up to recipes and data frames add_hp_space, add_hp_search and add_calibration would work the same way 5.3 Model Instantiation Let’s assume we’d like to use the KNN model family. In terms of implementation, I think things will be easiest if each model has a dedicated object initialization function. This function should return an object of class c(&quot;knn&quot;, &quot;model_family&quot;), with reasonable defaults in the hp_space, hp_search and calibration fields: knn_family &lt;- new_knn() But since the current paradigm in R doesn’t involve instantiating model objects before fitting them, I think it would also be good to provide a wrapper called knn that first creates a knn object and then fits it. That is, the following should all be equivalent: knn_fam_untrained &lt;- new_knn() knn_family &lt;- fit(knn_fam_untrained, design, data) knn_family &lt;- knn(design, data) knn_family &lt;- new_knn() %&gt;% fit(design, data) knn_family &lt;- new_knn() %&gt;% add_design(design, data) %&gt;% fit() 5.4 Model fitting To fit a model object, we could then do any of the following, returning an object of class c(&quot;knn&quot;, &quot;model_family&quot;). knn_model &lt;- knn(design, data, hp_space(k = 13, metric = &quot;euclidean&quot;)) knn_model &lt;- fit(new_knn(), design, data, hp_space = hp_space(k = 13, metric = &quot;euclidean&quot;)) knn_model &lt;- new_knn() %&gt;% add_design(recipe, data) %&gt;% add_hp_space(k = 13, metric = &quot;euclidean&quot;) %&gt;% fit() Since we are fitting a model rather than model_family here we don’t need to specify a hyperparameter search algorithm or a performance assessment specification. That is, you get a model back when there is a single set of hyperparameters in the hp_space and a model_family anytime the hp_space specifies multiple/infinite hyperparameter combinations. To fit model_family objects, the following would be equivalent knn_family &lt;- knn(design, data) # and showing default arguments knn_family &lt;- fit(model_family = new_knn(), # not a default argument! design = design, data = data, hp_space = default_knn_hp_space, hp_strategy = gaussian_process_opt, calibration = default_calibration) For users departing from the defaults, this might look like hyperparams &lt;- hp_space(k = 3:4, metric = c(&quot;euclidean&quot;, &quot;manhattan&quot;)) resamp_spec &lt;- calibration(score = &quot;mae&quot;, sampling = &quot;bootstrap&quot;, reps = 10) knn_family &lt;- new_knn() %&gt;% add_design(recipe, data) %&gt;% add_hp_space(hyperparams) %&gt;% add_hp_search(hyperband) %&gt;% add_resampling(resamp_spec) %&gt;% fit() If you wanted to do inference on the best model in knn_family, you could get it with best_knn_model &lt;- extract_model(knn_family) 5.5 Prediction Default predict methods should always return predictions of the same type as the input data. That is, if you specify a numeric outcome, you get a numeric prediction, if you specify a factor outcome, you get a factor prediction. This makes it easy for users to assess model performance, which is probably the first thing you want to do do after predicting. This would look like predictions &lt;- predict(knn_family, newdata) predictions &lt;- predict(best_knn_model, newdata) For sanity and consistency with Scikit-Learn, I think it would be good to add a new generic predict_proba to get class probabilities for classification problems class_probs &lt;- predict_proba(knn_family, newdata) 5.6 Shortcut methods TODO. things like fit_predict, fit_transform, fit_score, etc "],
["proposed-interface-using-models.html", "Part 6 Proposed interface: using models 6.1 Performance assessment", " Part 6 Proposed interface: using models 6.1 Performance assessment During the training process for a model family, the fit method will train a large number of models. Presumably it’d be nice to keep track of all these hyperparameter combinations in a tibble. We also want to keep track of model performance for each hyperparameter combination. To keep data tidy, this information should live in a separate table where rows correspond to unique models and columns correspond to performance on a particular resampled dataset (i.e. fold). In some cases users will want multiple or even many performance metrics for each fold. I think it makes sense for each metric to get it’s own results table? So users could do trained_model_family\\(rmse # get rmse results on all resampled datasets trained_model_family\\)mae # same, but for mae Presumably these tables should include some summary statistics, etc. Not sure how I feel about requiring the user to know the name of the metric to access the results tables. "],
["using-models-interactive-mode.html", "Part 7 Using models: interactive mode", " Part 7 Using models: interactive mode full fledged diagnostics visual model comparison in depth performance assessment (and visualizes) small scale prediction "],
["using-models-programmatic-automated-mode.html", "Part 8 Using models: programmatic / automated mode", " Part 8 Using models: programmatic / automated mode sanity checking a model / ML unit testing a la Google article prediction on new data (robust to data shittiness) methods(class=“lm”) also: augment, glance, tidy My thoughts up to this point have focused on a grammar of fitting models. I am increasingly interested in a grammar of interrogating models. In particular, I think that broom begins to provide a set of verbs for making it convenient to programmatically interrogate models. However, I think that there’s a lot more to do done, especially to define the conceptual interactions you want to have with a fitted model object. As one example, consider the workflow of something like astsa::sarima, where you always get a massive amount of information about: (1) convergence, (2) correct specification via residual analysis (residual ACF, QQ plot, Ljung-Box p values), (3) metrics like AIC, AICc, BIC, etc, (4) visual of the model fit. This is great to work with interactively since you immediately know if you fit a good model. On the other hand, predicting and programmatically interacting with astsa::sarima output is like pulling teeth because you have to recreate an astsa::sarima.for call with the same input to forecast for example. As another example of the tension between these two approaches, consider the general landscape for linear modelling, including penalized regression packages. The output for cv.glmnet and glm methods is drastically different despite researchers being interested in the same information in any cases (the coefficients for example). This suggests a number of different interrogation verbs may be necessary: assessment of convergence criteria comparison of performance metrics for difference hyperparameter values in depth assessment of predictive performance (like Harrell&#39;s many metrics in rms) assessment of model coefficients to do inference from looking at coefficient paths / inference at the model family level (that is, all of the verbs behind these ideas may behave different for models and for model families) anova / tidyposterior / resamples comparisons of multiple probe it with partial dependence plots, LIME, SHAP, other interpretability type stuff some sort of in-depth run_all_of_the_diagnostics metaverb to make interactive work convenient so you don&#39;t have to go back and forth a whole bunch a pick_the_best(model1, model2, ..., modeln, metrics = &quot;AICc&quot;) utility that returns the best model object? or is an intermediate comparison object needed? That seems more likely. These may have both numerical and visual summaries associated with them. Related: the many nice utilities in existing penalized regression packages how they would fit into a reimagined glm with penalized regression niceities fitted redundant since predict does the same thing when newdata = NULL in most contexts? Related: what I presume to be the standard set of methods for probing models in R: print summary plot coef residuals predict As a reference methods(class=“lm”) #&gt; [1] add1 alias anova case.names #&gt; [5] coerce confint cooks.distance deviance #&gt; [9] dfbeta dfbetas drop1 dummy.coef #&gt; [13] effects extractAIC family formula #&gt; [17] hatvalues influence initialize kappa #&gt; [21] labels logLik model.frame model.matrix #&gt; [25] nobs plot predict print #&gt; [29] proj qr residuals rstandard #&gt; [33] rstudent show simulate slotsFromS3 #&gt; [37] summary variable.names vcov #&gt; see ‘?methods’ for accessing help and source code fitted logLik Things that would be nice to do: likelihood ratio tests for mixed models Why don’t nls and lm have the same interface? Would it make sense to have access to these both at once? Garchfit diagnostics which arg to plot is evil Scikit-Learn provides the score method to quickly assess the performance of a model on new data. Is this a worthwhile thing to provide or too much from Python land and foreign? Could extract a performance metric from the calibration object and use that to get consistent behavior. In retrospect, maybe this is broom::glance? For a single model, it makes sense for broom::glance to return a single row of metrics. For a model family, would it be okay to return multiple rows, one for each set of hyperparameters? What are “silly things” in terms of modelling? Need someone with a bunch more background here, but the first things that spring to mind are: data leakage problems / poor hyperparameter validation / never assessing out of sample error hitting problems blindly with lots of models and getting stuck after that Can data leakage be at least partially solved by smart fit defaults? "],
["extension-to-unsupervised-learning.html", "Part 9 Extension to unsupervised learning 9.1 Invertible Mappings 9.2 Unsupervised Transformations That Are Not Maps", " Part 9 Extension to unsupervised learning This interface could also be extended to unsupervised learning following the spirit of Scikit-Learn by replacing the predict method with transform in the unsupervised case. Model families make sense in the unsupervised domain. For example, k-means and convex clustering both involve some sort of hyperparameter selection. For convex clustering you want to select a penalization parameter, probably optimizing one of several proposed clustering statistics For k-means you want to take the mode of cluster assignments, so transform(k_means) is probably a bad idea, but transform(k_means_family) would be interesting. 9.1 Invertible Mappings I want a standard S3 method for invertible mappings in the spirit of scale/unscale. In some sense recipes does this, but only for the forward transformation. scale/unscale store information about the transformation in the returned object, which I’m not sold on. I’d prefer the following: pca_model &lt;- pca(X) # pca(X) wraps fit(new_pca(), X) Z &lt;- transform(pca_model, X) # transform performs forward mapping X_recovered &lt;- untransform(pca_model, Z) # untransform performs inverse mapping and then you could provide the standard wrappers like fit_transform, fit_untransform, etc. 9.2 Unsupervised Transformations That Are Not Maps It’s worth thinking about unsupervised techniques like t-SNE because you can transform data, but you can’t ever fit a (non-parametric) t-SNE model because t-SNE doesn’t define a function mapping data to a new space. Offer a transform method for these but no fit? "],
["best-practices.html", "Part 10 Best Practices", " Part 10 Best Practices astsa as an example of interactively useful but programmatically a pain Show your example data in the README so users immediately see the structure Model classes beyond lists best practices: type definitions of methods for most functions as soon as there is a recipe interface for GLMs that’s all i’m ever going to use habit: get the df right, then y ~ . in the formula. would be nice to still see the features in the call? Every modeling function should include its package version in its data object I will now save my models as a list of three objects: model, data, and sessioninfo::session_info() Should be easy to get the values plotted so others can make their own plots modelling packages: - vignette should include not only the coefficients as output in an example, but also those coefficients written up as a general latex model and as a latex model with those specific coefficients substituted in TWO DISTINCT ISSUES THAT GET RESOLVED IN FORMULAE: design matrix specification model specification. (a la fGarch::garchFit(~arma(1, 1) + garch(1, 0))) where is the appropriate space in my framework to specify the arma and garch parameters? i.e. we specify these as models rather than model families, (although specifying one as a model family could be interesting as well) calls to fit should be pure: i.e. no side effects like plotting, and especially no plotting with invisible object return function to write out model form and fitted model in latex for sanity checking: some sort of model_report / model_form generic TODO: LASSO EXAMPLE of wanting to use the coefficients for prediction vs wanting to see the order in which features enter the model variable and model specification should happen in formulas, multiple formulas if necessary - need better tools (or documentation of existing tools): extracting and using the information found in formulae plm package: use of multiple formula sections in same formula to specify instrumental regression vs survey pacakge: use of multiple formulas in separate arguments "],
["existing-interfaces.html", "Part 11 Existing interfaces 11.1 Scikit-Learn 11.2 caret 11.3 mlr 11.4 Idiomatic modelling in R", " Part 11 Existing interfaces 11.1 Scikit-Learn Scikit-Learn knocks it out of the park in a couple areas: The API is consistent and easy to understand It’s easy to comply with the interface when designing new models or drop in replacements There are many extensions to facilitate ensembling and automatic hyperparameter selection Working with models is quite intuitive as well. For example, to fit a KNN model for classification with five neighbors, we could do the following: from sklearn import neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=5) knn.fit(X, y) knn.predict(X_test) However, Scikit-Learn doesn’t provide convenient abstractions for dealing with model families. Consider KNN where we would like to use random search to select a value of k, which looks something like: hyperparameter_space = { &#39;n_neighbors&#39;: sp_randint(1, 31) # pick k in [1, 2, ..., 30] } knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=hyperparameter_space) knn.fit(X, y) Here we have to create a random search object and wrap it around the KNN classifier. This hyperparameter search object now acts the original model. I find this somewhat confusing. Unfortunately, we have to manually specify the hyperparameter space, even though there are sane defaults. Additionally, different hyperparameter search objects accept different forms of hyperparameter specifications. For example, if we wanted to use a grid search, we’d need to pass in a list for n_neighbors, and if we wanted to use Tree Parzen Estimators from the hyperopt library, we’d have to use hyperopt’s custom hyperparameter space specifications. Another concern is that this doesn’t allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren’t doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select k. Instead we recalculate pairwise distances for each k, which is inefficient. Similarly, for penalized regressions, we don’t want to fit models for each value of the penalty parameter, we want to fit the entire solution path all at once. Scikit-Learn provides some work arounds to this. For example, with ridge regression, we can fit a RidgeCV object which efficiently performs cross-validation on the regularization parameter: from sklearn import linear_model reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0]) reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None, normalize=False) However, now we have to remember to call RidgeCV, resulting in a cluttered space of models that we need to keep track of. In any case, the result is that the code we write is tightly tied to our hyperparameter search method. This is somewhat brittle and I think will prove frustrating as the literature on hyperparameter search continues to grow. Additionally, beginners are most likely to use easy to understand yet inefficient methods such as grid search, since that code is the easiest to understand and provided in the examples. 11.1.0.1 Aside: automatic ML extensions Many of the Scikit-Learn extensions offer drop in classifiers or regressors. While these abstract the hyperparameter and model selection problems away from users, these systems tend to be designed for more hands off production use and are overly abstract at times. Consider the popular pipeline optimization package TPOT, which has the following interface from tpot import TPOTClassifier from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25) pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, random_state=42, verbosity=2) pipeline_optimizer.fit(X_train, y_train) In this case the user is working with an entire prediction pipeline, as opposed to a single model family. 11.1.0.2 Another aside: model instantiation In Scikit-Learn you have to instantiate a KNeighborsClassifier object and afterward call fit on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single call, like so: knn_model &lt;- knn_classifer(y ~ ., data, k = 5) 11.2 caret In my mind, the caret library most closely matches my intuition about working with model familys rather than models. library(caret) fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) knn_model &lt;- train(Species ~ ., data = iris, method = &quot;knn&quot;, trControl = fitControl) I like that the hyperparameter selection strategy is an argument (trControl) to the fit method, and I particularly like that each model comes with a default hyperparameter space specification. In the example above, caret automatically uses grid search on \\(\\k \\in \\{5, 7, 9}\\). Caret also takes advantage of built in, smart hyperparameter selection like cv.glmnet instead of manually checking values of \\(\\lambda\\). While caret partial abstracts away the hyperparameter selection problem, the default hyperparameter search is often not extensive enough to ignore hyperparameter selection. Users can pass in a data frame to specify a hyperparameter grid. However, caret only provides a limited number of built in hyperparameter search algorithms (grid search, grid search with racing and random search), so users have to write wrappers around to train to take advantage of bayesian optimization, for example. I think that well defined hyperparameter spaces, hyperparameter optimization functions, and arguments to specify both would go a long way toward improving ease of use. I would also like to see the specification of hyperparameter search algorithm separated from the resampling specification. Users can also use train to fit models rather than model families, by passing in a hyperparameter grid containing only a single point. I am not a fan of this, as I believe it blurs the distinct between models and model families. Caret is not tidy, and an occasion argument/function names can be difficult to grok at first. Lastly, caret does not have built in ensembling. There’s the caretEnsemble extension but I find the interface somewhat hard to use. The takeaway is that caret is a fantastic package that makes life a lot easier, but that it contains some design details that mean it probably isn’t an API to set as a communal standard for scientists producing packages. Aside: caret doesn’t require object instantiation like Scikit-Learn does. 11.3 mlr I’m including mlr here because it does contains lots of useful functionality, but I haven’t spent much time the package and don’t have a particularly principled critique. I find the interface rather unappealing, but can’t put my finger on exactly why. To use linear discriminant analysis on the iris dataset, you would: library(mlr) task = makeClassifTask(data = iris, target = &quot;Species&quot;) lrn = makeLearner(&quot;classif.lda&quot;) n = nrow(iris) train.set = sample(n, size = 2/3*n) test.set = setdiff(1:n, train.set) model = train(lrn, task, subset = train.set) pred = predict(model, task = task, subset = test.set) performance(pred, measures = list(mmce, acc)) In particular, I don’t like that I have to specify a task and I don’t like specifying the outcome variable via string. Through the mlrMBO extension we can use bayesian optimization techniques to define and search hyperparameter spaces, like so: library(mlrMBO) par.set = makeParamSet( makeDiscreteParam(&quot;kernel&quot;, values = c(&quot;radial&quot;, &quot;polynomial&quot;, &quot;linear&quot;)), makeNumericParam(&quot;cost&quot;, -15, 15, trafo = function(x) 2^x), makeNumericParam(&quot;gamma&quot;, -15, 15, trafo = function(x) 2^x, requires = quote(kernel == &quot;radial&quot;)), makeIntegerParam(&quot;degree&quot;, lower = 1, upper = 4, requires = quote(kernel == &quot;polynomial&quot;)) ) ctrl = makeMBOControl() ctrl = setMBOControlTermination(ctrl, iters = 5) tune.ctrl = makeTuneControlMBO(mbo.control = ctrl) res = tuneParams(makeLearner(&quot;classif.svm&quot;), iris.task, cv3, par.set = par.set, control = tune.ctrl, show.info = FALSE) This is definitely functionality that I want, but there are a lot of different objects happening at the same time in the workspace, and I don’t understand why this is necessary. I also think some of the boilerplate should disappear and models should be provided with default hyperparameter spaces. I suppose that my big complaint is that I would rather work with model and model families objects rather than a set of controller objects. 11.4 Idiomatic modelling in R Let’s consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so library(lasso) lasso_object &lt;- lasso(X, y, lambda = 0.01) predict(lasso_object, X_new) Since there are efficient ways to cross-validate \\(\\lambda\\) for lasso regression, the package would likely also implement an interface like lasso_cv_object &lt;- lasso_cv(X, y) predict(lasso_cv_object) that would automatically select an optimal value of \\(\\lambda\\). A nice package author would make lasso and lasso_cv generics and would also implement formula or even recipe interfaces to the model, like so lasso_object &lt;- lasso(y ~ ., data = df, lambda = 0.01) lasso_cv_object &lt;- lasso_cv(y ~ ., data = df) I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because: When there isn’t a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with. That is, most of the time people work with multiple models, so it is incredibly convenient to be able to do something like: model_familys &lt;- list(lasso(), ridge(), OLS()) train_models &lt;- map(models, ~fit(model_familys, y ~ ., data)) but this isn’t possible because each function has it’s own version of fit. Unless there’s a recipe interface to the lasso_cv function there isn’t a way to do principled preprocessing when resampling to estimate prediction error Feature creep inevitably means that individual packages add resampling, hyperparameter search and preprocessing functionality to the lasso and lasso_cv functions, making it difficult to extend them or write modular code "]
]
