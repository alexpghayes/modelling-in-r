[
["index.html", "Some thoughts on modelling in R Part 1 Motivation", " Some thoughts on modelling in R Alex Hayes 2017-12-29 Part 1 Motivation Each model in R essentially lives in its own package and has a unique interface. This introduces a large amount of cognitive load on data analysts. For example, suppose we want to use KNN. We might do something like this: library(tidyverse) library(rsample) data &lt;- initial_split(iris) train &lt;- training(data) test &lt;- select(testing(data), -Species) knn_preds &lt;- class::knn(test = test, train = select(train, -Species), cl = train$Species, k = 5) But if we want to use naive Bayes, we might end up writing code that looks like this: nb_model &lt;- e1071::naiveBayes(Species ~ ., data = train) nb_preds &lt;- predict(nb_model, newdata = test) This has some problems: knn generates predictions immediately on a test set, while naiveBayes creates a model object For knn we have to pass arguments cl and k even though it would be reasonable to select k by cross-validation and cl could be more succinctly expressed as an outcome in a formula knn and naiveBayes have different interfaces for specifying design matrices and outcomes knn and naiveBayes both return factor predictions by default, but this might not be the case for other packages. If we want to class probabilities, we have to pass prob = TRUE to knn and type = &quot;raw&quot; to predict.naiveBayes, and the outputs are in entirely different formats. That is, there isn’t a consistent interface to the packages themselves. Additionally, the packages don’t make use of a conceptual framework that makes it easy to think about modelling. The goal of the document is provide a grammar of modelling that resolves both of these problems. "],
["models-and-model-families.html", "Part 2 Models and model families", " Part 2 Models and model families When someone talks about supervised learning, they might say something like: “I fit K Nearest Neighbors on the dataset and got 57 percent accuracy.” This communicates the gist of what they did, but this isn’t enough information to reproduce what they actually did: we don’t know what value of \\(k\\) or the distance metric they used. When someone says that they used K Nearest Neighbors, they are referring to a family of supervised learning models, that is, KNN with \\(k \\in \\{1, 2, ...\\}\\) and some distance metric from a list of metrics, say, the \\(\\ell_p\\) metrics. Together all the possible values of \\(k\\) combined with all the \\(\\ell_p\\) metrics form a hyperparameter space, and we have to hope that our friend has selected hyperparameters in some reasonable way. Once our friend has selected hyperparameters, say \\(k=13\\) and Euclidean distance, they are now speaking about a specific, unambiguous model. With these hyperparameters, we have enough information to fit exactly the same model ourselves. These objects form the basis for our grammar: model family = modelling technique + hyperparameter space model = modelling technique + specific values of hyperparameters As a concrete example, consider the glmnet package. glmnet objects fit with a specific value of \\(\\lambda\\) correspond to model objects cv.glmnet objects correspond to a model family containing performance information for various hyperparameter values Once fit, a model family is a set of fit models. There are important differences between model family objects and model objects. For example, we typically perform inference on models rather than model familys. "],
["fitting-model-families.html", "Part 3 Fitting model families", " Part 3 Fitting model families Fitting a model is quite different from fitting a model family. To fit a model, we need: Data Specific hyperparameter values A way to train the model given data and specific hyperparameters On the other hand, to fit a model family, we need: Data The hyperparameter space to consider A way to train the model given data and specific hyperparameters A way to search through hyperparameter space A way to determine which trained model is best To find the best model in the model family, we need a performance metric, such as root mean squared error, and an estimate of this metric on out-of-sample model data. This likely means getting multiple estimates of model performance by fitting the same model on resampled datasets. Keeping models and model families in mind, let’s think about some features that would make a modelling interface intuitive and useful to users. "],
["interface-desirables.html", "Part 4 Interface desirables", " Part 4 Interface desirables Uses as much language from Scikit-Learn as possible Since Scikit-Learn is likely the most uniform and widely known interface for predictive modelling, this reduces cognitive load. Additionally, the Python ML world has a strong tradition of implementing new models with the Scikit-Learn interface that we hope will carry over to new researchers implementing new methods in R. Researchers can easily implement new models that conform to the interface This is because we often want to use multiple models at the same time, and it’s a nightmare when these models don’t share the same API. Each model and model family corresponds to a single distinct class This makes it easy to provide an interface compliant template that researchers can will fill in with new models they implement. It also allows them to write custom S3 methods, which they likely expect, and to provide smart cross-validation schemes when appropriate. Ensembling is easy Most users interested in prediction will create an ensemble at some point. To appeal to the Kaggle crowd as much as possible, it should be easy to bag, boost and stack models to arbitrary depths. Additionally, a number of R packages use the SuperLearner package as the basis for Super Learner and semi-parametric inference. If new machine learning techniques implement a standardized interface, then packages like SuperLearner (and also caret and mlr for that matter) don’t need to implement wrappers for each new model. Facilitates automated machine learning Recently industrial data science has been tending toward automating as many redundant parts of the modelling process as possible. Take for example: H2O’s Automated Machine Learning, which has already landed in R with h2o.automl() H2O’s Driverless AI Airbnb’s modelling workflow TPOT mlrMBO Spearmint In terms of implementation, this means it should be easy to programmatically generate and manipulate model and model familys. This also means that hyperparameter space specifications should be flexible and informative enough to handle new hyperparameter search algorithms, and that it should be easy to specify smart defaults. I think it would also be nice to “metalearn” good hyperparameters to try initially, as in auto-sklearn. Tidy and pipeable data structures The interface should conform to tidyverse principles, given the consistent standards and wide user adoption of the tidyverse. Users think in terms of model familys Since it’s most intuitive to think about model families, we want the interface to reflect this. For operations that require an actual model, such as prediction, users should be able to pass in model family objects, and then have the software automatically extract and use the best model in the model family. The interface should provide a clear idea of what it means to fit a model family, so that interested users can customize hyperparameter spaces and selection techniques. Keeping these desirables in mind, let’s think about how existing machine learning libraries work. "],
["existing-interfaces.html", "Part 5 Existing interfaces 5.1 Scikit-Learn 5.2 caret 5.3 mlr 5.4 Idiomatic modelling in R", " Part 5 Existing interfaces 5.1 Scikit-Learn Scikit-Learn knocks it out of the park in a couple areas: The API is consistent and easy to understand It’s easy to comply with the interface when designing new models or drop in replacements There are many extensions to facilitate ensembling and automatic hyperparameter selection Working with models is quite intuitive as well. For example, to fit a KNN model for classification with five neighbors, we could do the following: from sklearn import neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=5) knn.fit(X, y) knn.predict(X_test) However, Scikit-Learn doesn’t provide convenient abstractions for dealing with model families. Consider KNN where we would like to use random search to select a value of k, which looks something like: hyperparameter_space = { &#39;n_neighbors&#39;: sp_randint(1, 31) # pick k in [1, 2, ..., 30] } knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=hyperparameter_space) knn.fit(X, y) Here we have to create a random search object and wrap it around the KNN classifier. This hyperparameter search object now acts the original model. I find this somewhat confusing. Unfortunately, we have to manually specify the hyperparameter space, even though there are sane defaults. Additionally, different hyperparameter search objects accept different forms of hyperparameter specifications. For example, if we wanted to use a grid search, we’d need to pass in a list for n_neighbors, and if we wanted to use Tree Parzen Estimators from the hyperopt library, we’d have to use hyperopt’s custom hyperparameter space specifications. Another concern is that this doesn’t allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren’t doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select k. Instead we recalculate pairwise distances for each k, which is inefficient. Similarly, for penalized regressions, we don’t want to fit models for each value of the penalty parameter, we want to fit the entire solution path all at once. Scikit-Learn provides some work arounds to this. For example, with ridge regression, we can fit a RidgeCV object which efficiently performs cross-validation on the regularization parameter: from sklearn import linear_model reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0]) reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None, normalize=False) However, now we have to remember to call RidgeCV, resulting in a cluttered space of models that we need to keep track of. In any case, the result is that the code we write is tightly tied to our hyperparameter search method. This is somewhat brittle and I think will prove frustrating as the literature on hyperparameter search continues to grow. Additionally, beginners are most likely to use easy to understand yet inefficient methods such as grid search, since that code is the easiest to understand and provided in the examples. 5.1.0.1 Aside: automatic ML extensions Many of the Scikit-Learn extensions offer drop in classifiers or regressors. While these abstract the hyperparameter and model selection problems away from users, these systems tend to be designed for more hands off production use and are overly abstract at times. Consider the popular pipeline optimization package TPOT, which has the following interface from tpot import TPOTClassifier from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25) pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, random_state=42, verbosity=2) pipeline_optimizer.fit(X_train, y_train) In this case the user is working with an entire prediction pipeline, as opposed to a single model family. 5.1.0.2 Another aside: model instantiation In Scikit-Learn you have to instantiate a KNeighborsClassifier object and afterward call fit on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single call, like so: knn_model &lt;- knn_classifer(y ~ ., data, k = 5) 5.2 caret In my mind, the caret library most closely matches my intuition about working with model familys rather than models. library(caret) fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) knn_model &lt;- train(Species ~ ., data = iris, method = &quot;knn&quot;, trControl = fitControl) I like that the hyperparameter selection strategy is an argument (trControl) to the fit method, and I particularly like that each model comes with a default hyperparameter space specification. In the example above, caret automatically uses grid search on \\(\\k \\in \\{5, 7, 9}\\). Caret also takes advantage of built in, smart hyperparameter selection like cv.glmnet instead of manually checking values of \\(\\lambda\\). While caret partial abstracts away the hyperparameter selection problem, the default hyperparameter search is often not extensive enough to ignore hyperparameter selection. Users can pass in a data frame to specify a hyperparameter grid. However, caret only provides a limited number of built in hyperparameter search algorithms (grid search, grid search with racing and random search), so users have to write wrappers around to train to take advantage of bayesian optimization, for example. I think that well defined hyperparameter spaces, hyperparameter optimization functions, and arguments to specify both would go a long way toward improving ease of use. I would also like to see the specification of hyperparameter search algorithm separated from the resampling specification. Users can also use train to fit models rather than model families, by passing in a hyperparameter grid containing only a single point. I am not a fan of this, as I believe it blurs the distinct between models and model families. Caret is not tidy, and an occasion argument/function names can be difficult to grok at first. Lastly, caret does not have built in ensembling. There’s the caretEnsemble extension but I find the interface somewhat hard to use. The takeaway is that caret is a fantastic package that makes life a lot easier, but that it contains some design details that mean it probably isn’t an API to set as a communal standard for scientists producing packages. Aside: caret doesn’t require object instantiation like Scikit-Learn does. 5.3 mlr I’m including mlr here because it does contains lots of useful functionality, but I haven’t spent much time the package and don’t have a particularly principled critique. I find the interface rather unappealing, but can’t put my finger on exactly why. To use linear discriminant analysis on the iris dataset, you would: library(mlr) task = makeClassifTask(data = iris, target = &quot;Species&quot;) lrn = makeLearner(&quot;classif.lda&quot;) n = nrow(iris) train.set = sample(n, size = 2/3*n) test.set = setdiff(1:n, train.set) model = train(lrn, task, subset = train.set) pred = predict(model, task = task, subset = test.set) performance(pred, measures = list(mmce, acc)) In particular, I don’t like that I have to specify a task and I don’t like specifying the outcome variable via string. Through the mlrMBO extension we can use bayesian optimization techniques to define and search hyperparameter spaces, like so: library(mlrMBO) par.set = makeParamSet( makeDiscreteParam(&quot;kernel&quot;, values = c(&quot;radial&quot;, &quot;polynomial&quot;, &quot;linear&quot;)), makeNumericParam(&quot;cost&quot;, -15, 15, trafo = function(x) 2^x), makeNumericParam(&quot;gamma&quot;, -15, 15, trafo = function(x) 2^x, requires = quote(kernel == &quot;radial&quot;)), makeIntegerParam(&quot;degree&quot;, lower = 1, upper = 4, requires = quote(kernel == &quot;polynomial&quot;)) ) ctrl = makeMBOControl() ctrl = setMBOControlTermination(ctrl, iters = 5) tune.ctrl = makeTuneControlMBO(mbo.control = ctrl) res = tuneParams(makeLearner(&quot;classif.svm&quot;), iris.task, cv3, par.set = par.set, control = tune.ctrl, show.info = FALSE) This is definitely functionality that I want, but there are a lot of different objects happening at the same time in the workspace, and I don’t understand why this is necessary. I also think some of the boilerplate should disappear and models should be provided with default hyperparameter spaces. I suppose that my big complaint is that I would rather work with model and model families objects rather than a set of controller objects. 5.4 Idiomatic modelling in R Let’s consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so library(lasso) lasso_object &lt;- lasso(X, y, lambda = 0.01) predict(lasso_object, X_new) Since there are efficient ways to cross-validate \\(\\lambda\\) for lasso regression, the package would likely also implement an interface like lasso_cv_object &lt;- lasso_cv(X, y) predict(lasso_cv_object) that would automatically select an optimal value of \\(\\lambda\\). A nice package author would make lasso and lasso_cv generics and would also implement formula or even recipe interfaces to the model, like so lasso_object &lt;- lasso(y ~ ., data = df, lambda = 0.01) lasso_cv_object &lt;- lasso_cv(y ~ ., data = df) I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because: When there isn’t a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with. That is, most of the time people work with multiple models, so it is incredibly convenient to be able to do something like: model_familys &lt;- list(lasso(), ridge(), OLS()) train_models &lt;- map(models, ~fit(model_familys, y ~ ., data)) but this isn’t possible because each function has it’s own version of fit. Unless there’s a recipe interface to the lasso_cv function there isn’t a way to do principled preprocessing when resampling to estimate prediction error Feature creep inevitably means that individual packages add resampling, hyperparameter search and preprocessing functionality to the lasso and lasso_cv functions, making it difficult to extend them or write modular code "],
["proposed-interface.html", "Part 6 Proposed interface 6.1 Objects in play 6.2 Model Instantiation 6.3 Model fitting 6.4 Prediction", " Part 6 Proposed interface Okay, now that we’ve taken a look at some interfaces, let’s imagine an interface that makes operations on model and model familys feel natural in R. 6.1 Objects in play Recall that to fit a model, we need: Data Specific hyperparameter values A way to train the model given data and specific hyperparameters On the other hand, to fit a model family, we need: Data The hyperparameter space to consider A way to train the model given data and specific hyperparameters A way to search through hyperparameter space A way to determine which trained model is best Our task is now to design intuitive ways to specify all of these. Thankfully, Max Kuhn has already solved several of these problems for us: The recipes package creates maps from messy input data to design matrices, generalizing the formula. The learned map can then be applied to new data. The yardstick package provides tidy calculations of various performance metrics given predictions and the baseline truth The tidyposterior package provides methods to compare models within a model family by comparing resampled performance metrics The rsample package provides infrastructure for a variety of resampling strategies (although does not provide a way to specify a resampling scheme beyond raw rsets) CRAN provides packages to fit most models of interest. This leaves us with a couple remaining problems that we will assume have nice solutions for the moment. 6.1.1 Model calibration We need to be able to find the best model in a given model family. For the sake of this document, we’ll assume there’s an imaginary calibration object that consists of: (1) a resampling specification, (2) a performance metric and (3) an appropriate strategy for comparing performance metrics. As a concrete example, a calibration object might specify that each model in the model should be fit on 20 bootstrap samples, and the best model would have the lowest average training RMSE on resamples datasets. 6.1.2 Hyperparameter space definition For now, I’m going to assume that the problem of hyperparameter space definition has been solved, and that there are nice hp_space objects that contain this information. 6.1.3 Hyperparameter search Similarly, let’s assume that there are standard functions for searching through hyperparameter space of class hp_search. See the github issues for some thoughts on what these might look like. 6.1.4 Model and model family objects 6.1.4.1 model trained: a logical indicating if the model has been fit design: a recipe specifying a transformation into a design matrix hyperparameters: a named list of hyperparameters to fit the model 6.1.4.2 model_family trained: a logical indicating if the model has been fit design: a recipe specifying a transformation into a design matrix hp_space: a hp_space object hp_search: a hp_search object calibration: a calibration object 6.1.4.3 Pipeable helpers Each of the following would accept a model or model_family object and update the appropriate field: add_design would return a model with an updated design field. It would nice for this to be a generic that also had matrix and formula methods that promoted data up to recipes and data frames add_hp_space, add_hp_search and add_calibration would work the same way 6.2 Model Instantiation Let’s assume we’d like to use the KNN model family. In terms of implementation, I think things will be easiest if each model has a dedicated object initialization function. This function should return an object of class c(&quot;knn&quot;, &quot;model_family&quot;), with reasonable defaults in the hp_space, hp_search and calibration fields: knn_family &lt;- new_knn() But since the current paradigm in R doesn’t involve instantiating model objects before fitting them, I think it would also be good to provide a wrapper called knn that first creates a knn object and then fits it. That is, the following should all be equivalent: knn_fam_untrained &lt;- new_knn() knn_family &lt;- fit(knn_fam_untrained, design, data) knn_family &lt;- knn(design, data) knn_family &lt;- new_knn() %&gt;% fit(design, data) knn_family &lt;- new_knn() %&gt;% add_design(design, data) %&gt;% fit() 6.3 Model fitting To fit a model object, we could then do any of the following, returning an object of class c(&quot;knn&quot;, &quot;model_family&quot;). knn_model &lt;- knn(design, data, hp_space(k = 13, metric = &quot;euclidean&quot;)) knn_model &lt;- fit(new_knn(), design, data, hp_space = hp_space(k = 13, metric = &quot;euclidean&quot;)) knn_model &lt;- new_knn() %&gt;% add_design(recipe, data) %&gt;% add_hp_space(k = 13, metric = &quot;euclidean&quot;) %&gt;% fit() Since we are fitting a model rather than model_family here we don’t need to specify a hyperparameter search algorithm or a performance assessment specification. That is, you get a model back when there is a single set of hyperparameters in the hp_space and a model_family anytime the hp_space specifies multiple/infinite hyperparameter combinations. To fit model_family objects, the following would be equivalent knn_family &lt;- knn(design, data) # and showing default arguments knn_family &lt;- fit(model_family = new_knn(), # not a default argument! design = design, data = data, hp_space = default_knn_hp_space, hp_strategy = gaussian_process_opt, calibration = default_calibration) For users departing from the defaults, this might look like hyperparams &lt;- hp_space(k = 3:4, metric = c(&quot;euclidean&quot;, &quot;manhattan&quot;)) resamp_spec &lt;- calibration(score = &quot;mae&quot;, sampling = &quot;bootstrap&quot;, reps = 10) knn_family &lt;- new_knn() %&gt;% add_design(recipe, data) %&gt;% add_hp_space(hyperparams) %&gt;% add_hp_search(hyperband) %&gt;% add_resampling(resamp_spec) %&gt;% fit() If you wanted to do inference on the best model in knn_family, you could get it with best_knn_model &lt;- extract_model(knn_family) 6.4 Prediction Default predict methods should always return predictions of the same type as the input data. That is, if you specify a numeric outcome, you get a numeric prediction, if you specify a factor outcome, you get a factor prediction. This makes it easy for users to assess model performance, which is probably the first thing you want to do do after predicting. This would look like predictions &lt;- predict(knn_family, newdata) predictions &lt;- predict(best_knn_model, newdata) For sanity and consistency with Scikit-Learn, I think it would be good to add a new generic predict_proba to get class probabilities for classification problems class_probs &lt;- predict_proba(knn_family, newdata) "],
["extension-to-unsupervised-learning.html", "Part 7 Extension to unsupervised learning 7.1 Invertible Mappings", " Part 7 Extension to unsupervised learning This interface could also be extended to unsupervised learning following the spirit of Scikit-Learn by replacing the predict method with transform in the unsupervised case. Model families make sense in the unsupervised domain. For example, k-means and convex clustering both involve some sort of hyperparameter selection. For convex clustering you want to select a penalization parameter, probably optimizing one of several proposed clustering statistics For k-means you want to take the mode of cluster assignments, so transform(k_means) is probably a bad idea, but transform(k_means_family) would be interesting. 7.1 Invertible Mappings I want a standard S3 method for invertible mappings in the spirit of scale/unscale. In some sense recipes does this, but only for the forward transformation. scale/unscale store information about the transformation in the returned object, which I’m not sold on. I’d prefer the following: pca_model &lt;- pca(X) # pca(X) wraps fit(new_pca(), X) Z &lt;- transform(pca_model, X) # transform performs forward mapping X_recovered &lt;- untransform(pca_model, Z) # untransform performs inverse mapping and then you could provide the standard wrappers like fit_transform, fit_untransform, etc. "]
]
