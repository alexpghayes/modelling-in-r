# Extension to unsupervised learning

This interface could also be extended to unsupervised learning following the spirit of `Scikit-Learn` by replacing the `predict` method with `transform` in the unsupervised case.

I also think there's an important equivalent of a `model family` in the unsupervised domain. For example, k-means and convex clustering both involve some sort of hyperparameter selection.

- For convex clustering you want to select a penalization parameter according to some strategy, probably optimizing one of several proposed clustering statistics
- For k-means you want to take the mode of cluster assignments, so `predict(knn)` is probably a bad idea, while `predict(knn_family)` is more reasonable in practice. I don't know of any R packages that have the functionality you'd assume from `predict(knn_family)`, I presume you always have to implement it yourself. I know `h2o` automatically selects `k`, but I'm not sure how the actual cluster assignments come about. I presume it's reasonable though.

## Pipelines

`Scikit-Learn` offers pipelines that allow you to do things like do PCA on data and then fit a GBM to that data, but with an arbitrary number of steps. I'm curious how this should work with the current direction of modelling in R, where `recipes` would presumably take care of the `PCA`. What if you want to train a model on the GBM predictions? Do you need a new `pipeline` object, or do you make a `step_gbm` for a recipe?

## Non-repeatable mappings

It's worth thinking about unsupervised techniques like `t-SNE` because you can `transform` data, but you can't ever `fit` a `t-SNE` object because (non-parametric) `t-SNE` doesn't define a mapping to a new space.

## Invertible Mappings

I would like a standard  (S3 method?) for invertible mappings in the spirit of `scale`/`unscale`. In some sense recipes does this, but only for the forward transformation. `scale`/`unscale` store information about the transformation in the returned object, which I'm not sold on. I'd prefer something like:

```{r}
pca_model <- pca(X)                 # recall pca(X) wraps fit(new_pca(), X)
Z <- transform(pca, X)              # transform performs forward mapping
X_recovered <- untransform(pca, Z)  # untransform performs inverse mapping

# TODO: "model" probably isn't the best name for unsupervised transformations
# even if those transformation do end up having "model"/"model_family" class
```

**Side note**: I'd love to see a wrapper around PCA that easily lets the user specify a number of principal components to drop into `irlba` for high dimensional situations when computing the full PCA is overkill.
