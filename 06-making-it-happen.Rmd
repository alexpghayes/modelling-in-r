# How to make this happen

If the R community did accept these as communal standards for modelling, what steps would we need to take.

## Roadblocks/needs before standardizing a modelling interface to community standardHow to implement new models

I think it's particular important to establish community standards for how to implement a `recipes` interface when you come up with a new model. How do you deal with intercepts for example? I'm hoping to make a PR to the `recipes` package with a `detect_step` function to simplify this, but I'm not sure if there are any recommendations on how or why to use this type of functionality.

Related: a detailed tutorial for package developers describing how to conveniently deal with all three of the following data formats:

- `X` matrix, `y` vector/matrix response
- `formula` formula and `data` data frame
- `recipe` recipe and `data` data frame

Similarly, I'd like to see `rsample` export an object describing a resampling scheme similar to `fitControl` so that researchers can implement consistent resampling interfaces across packages. For example, I'm currently helping out with a package that includes a large number of options specifying cross-validation controls that could all be wrapper together and made consistent across packages. (TODO: add `lariat` package details?). **Question**: Is an `rset` object sufficient specification? Should `fit` methods accept `rset` objects rather than data frames? Or is a specification type deal better because of likely resistance to `rset` objects? That, whic of the following is going to be more accepted:

- `data` data frame, `fitControl` equivalent specification
- `rset` data only. Does this imply an additional line of code going from the data to the `rset`? How does this influence workflow?

Similarly, many modelling packages currently mix pre-processing and model fitting. Intercepts again come up as a sticky issue. If you want to be able to specify an intercept while providing both a `recipe`/`data` and `X`/`y` interface, you probably have to create a default `fit(model, X, y, intercept)` method and then a `fit(model, recipe, data)` that creates `X` and `y` and infers `intercept`. I find this unsatisfying and inelegant. The best solution seems to be making the `recipe` interface the standard, but I imagine a huge amount of push back on this (i.e. all my professors who say thinks like "I've never really gotten the point of data frames."). Similarly, many regression packages provide arguments that all users to specify:

- if data should be centered
- if data should be scaled
- if some sort of dimension reduction should be applied to data
- how to deal with missing data

I strongly believe the `recipes` package should handle all of the above. That way code can be more modular and consistent.

## Infrastructure to provide

- Skeleton package defining general inference and explaining where to fill in the details for a particular modelling method
- Providing a system to check for interface compliance
- Provide an interface compliant machine learning library to demonstrate how nice the interface is

## Misc

How to incentivize busy profs to rewrite packages they no longer have time to think about?

How far can GSOC and PR go?
