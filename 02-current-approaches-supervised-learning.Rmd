# Existing approaches to supervised learning

## `Scikit-Learn`

In Scikit-Learn we can fit and predict with `model` objects quite intuitively:

```{python}
from sklearn import neighbors
knn = neighbors.KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)
predictions = knn.predict(X_test)
```

However, I find the abstractions to work with `model family`s less satisfying. Considering the KNN model family where we use random search to select a value of $k$, which looks something like:

```{python}
hyperparameter_space = {
    'n_neighbors': sp_randint(1, 31)  # pick k in [1, 2, ..., 30]
}

knn = RandomizedSearchCV(KNeighborsClassifier(),
                         param_distributions=hyperparameter_space)

knn.fit(X, y)
```

I find this suboptimal for a couple reasons:

- We have to manually specify the hyperparameter space even though there are sane defaults
- If we wanted to use grid search or a different hyperparameter selection method, we'd have to change the specification of the hyperparameter space. That is, We'd have to pass a list of values for `n_neighbors` rather than a distribution function for a grid search.
- This doesn't allow us to take advantage of structure in the hyperparameter space. For example, with KNN, assuming we aren't doing any fancy approximation methods, we really want to calculate pairwise distances exactly once, and then reuse that pairwise distance information to select $k$. Instead we recalculate pairwise distances for each $k$, which is inefficient. 

In some cases `sklearn` provides work arounds to this, for example, with RIDGE regression:

```{python}
from sklearn import linear_model
reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
```

Here we fit a `RidgeCV` object which efficiently performs cross-validation on the regularization parameter. However, now we have to remember to call `RidgeCV` rather than the standard grid search wrapper.

It would be cleaner to specify that we want to work with the KNN/RIDGE model family and then plug in whichever hyperparameter selection technique we choose via the same interface.

There are some things that `Scikit-Learn` does really well though:

- It has an extremely consistent interface that has been established as a communal standard within the Python community
- There are extension libraries that make automated machine learning and ensemble creation easy and pleasant

Lastly, it's worth noting that in `Scikit-Learn` you have to instantiate a `KNeighborsClassifier` object and afterward call `fit` on it. This differs from R where a dominant paradigm is to instantiate a model and train it with a single call, like so:

```{r}
knn_model <- knn_classifer(y ~ ., data, k = 5)
```


## `caret`

In my mind, the `caret` library most closely matches my intuition about working with `model family`s rather than `model`s.

```{r}
library(caret)

# specify 10-fold CV repeated 10 times are hyperparameter selection strategy
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

knn_model <- train(y ~ .,
                   data = train_df, 
                   method = "knn", 
                   trControl = fitControl)
```

Some things I like about the `caret` interface:

- The hyperparameter selection strategy is an argument to the fit method
- Reasonable defaults are provided for some hyperparameters, so it occasionally does feel like you're working with a `model family` that has abstracted away hyperparameter selection
- `caret` takes advantage of built in, smart hyperparameter selection like `cv.glmnet` instead of manually checking values of $\lambda$

Some things I don't like about the `caret` interface:

- The default hyperparameter search is not an extensive enough to ignore hyperparameter selection in the majority of cases and so you end up specifying a tuning grid most of the time. Ideally I think all models should function with default hyperparameter selection as in `h2o.automl()`
- The result of `train` is an object of class `train`. That is, you know it's a model family object, but the precise modelling technique is stored as a string. This means you have to write wrapper code if you want to add a model to the `caret` interface (i.e. it's a nice package to work with, but it probably isn't a great API to set as a communal standard for scientist producing packages)
- Ensembling is hard. There's the `caretEnsemble` extension but I find the interface a messy.
- I find the names of several functions and function arguments unintuitive.

It's also worth noting that `caret` doesn't require object instantiation like `Scikit-Learn` does.

## `mlr`

Briefly, the `mlr` library is similar to `caret`, but with the following interface:

```{r}
task = makeClassifTask(data = iris, target = "Species")
lrn = makeLearner("classif.lda")

n = nrow(iris)
train.set = sample(n, size = 2/3*n)
test.set = setdiff(1:n, train.set)

model = train(lrn, task, subset = train.set)

pred = predict(model, task = task, subset = test.set)
performance(pred, measures = list(mmce, acc))
```

I think this interface has issues similar to `caret`s interface.

Most importantly, however, I believe that the primary focus of supervised learning should be `model family`s, as opposed to `task`s. I strongly believe the task at hand should be inferred from the class of the data object.

I think this is important because we often think about inference in terms of `model`s but almost never in terms of `task`s.

## Idiomatic modelling in R

Let's consider an imaginary package using an idiomatic R interface that performs lasso regression. A nicely written package might have an interface like so

```{r}
library(lasso)

lasso_object <- lasso(X, y, lambda = 0.01)
predict(lasso_object, X_new)
```

Since there are efficient ways to cross-validate $\lambda$ for lasso regression, the package would likely also implement an interface like

```{r}
lasso_cv_object <- lasso_cv(X, y)
predict(lasso_cv_object)
```

that would automatically select an optimal value of $\lambda$. A nice package author would make `lasso` and `lasso_cv` generics and would also implement formula or even [recipe](https://github.com/topepo/recipes) interfaces to the model, like so

```{r}
lasso_object <- lasso(y ~ ., data = df, lambda = 0.01)
lasso_cv_object <- lasso_cv(y ~ ., data = df)
```

I think this is a clean interface and a good standard to keep until something better comes along. In the long term, I would like the community standard for modelling to change, because:

- When there isn't a smart way to select to perform cross-validation, you have to write hyperparameter search code yourself, and small variations in interface design mean you have to do this for each different model you work with. That is, most of the time people work with multiple models, so it is incredibly convenient to be able to do something like:

```{r}
model_familys <- list(lasso(), ridge(), OLS())
train_models <- map(models, ~fit(model_familys, y ~ ., data))
```

but this isn't possible because each function has it's own version of `fit`.

- Unless there's a `recipe` interface to the `lasso_cv` function there isn't a way to do principled preprocessing when resampling to estimate prediction error
